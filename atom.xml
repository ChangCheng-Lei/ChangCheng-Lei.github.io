<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>ChangCheng</title>
  
  <subtitle>ChangCheng Blog</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2019-07-05T06:24:26.200Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>[object Object]</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Spark学习（二）RDD详解</title>
    <link href="http://yoursite.com/2019/06/23/spark-rdd-programming/"/>
    <id>http://yoursite.com/2019/06/23/spark-rdd-programming/</id>
    <published>2019-06-23T08:18:16.000Z</published>
    <updated>2019-07-05T06:24:26.200Z</updated>
    
    <content type="html"><![CDATA[<div class="toc"><!-- toc --><ul><li><a href="#rdd-jian-jie">RDD 简介</a><ul><li><a href="#ji-ben-gai-nian">基本概念</a></li><li><a href="#chuang-jian-rdd">创建RDD</a><ul><li><a href="#parallelized-collections">Parallelized Collections</a></li><li><a href="#external-datasets-cong-wai-bu-wen-jian-du-qu">External Datasets 从外部文件读取</a></li></ul></li></ul></li><li><a href="#rdd-cao-zuo">RDD 操作</a><ul><li><a href="#transformation">Transformation</a><ul><li><a href="#flatmap-he-map">flatMap 和 Map</a></li><li><a href="#sample-cao-zuo">sample 操作</a></li><li><a href="#distinct-cao-zuo">distinct 操作</a></li><li><a href="#union-he-bing-cao-zuo">union 合并操作</a></li><li><a href="#intersection-jiao-ji">intersection 交集</a></li><li><a href="#substract-zi-ji">substract 子集</a></li><li><a href="#cartesian-di-qia-er-ji">cartesian 笛卡尔积</a></li></ul></li><li><a href="#actions">Actions</a><ul><li><a href="#collect-cao-zuo">collect 操作</a></li><li><a href="#count-he-countbyvalue">count 和 countByValue</a></li><li><a href="#take-cao-zuo">take 操作</a></li><li><a href="#saveastextfile-cao-zuo">saveAsTextFile 操作</a></li><li><a href="#reduce-cao-zuo">reduce 操作</a></li></ul></li></ul></li></ul><!-- tocstop --></div><h2><span id="rdd-jian-jie">RDD 简介</span><a href="#rdd-jian-jie" class="header-anchor"></a></h2><blockquote><p>Resilient Distributed Dataset 意思是弹性分布式数据集,是Spark 中最基本的数据抽象</p></blockquote><p>本文所有源代码可以访问<a href="https://github.com/ChangCheng-Lei/Java-Spark-Tutorial" target="_blank" rel="noopener">GIT HUB</a> 获取</p><h3><span id="ji-ben-gai-nian">基本概念</span><a href="#ji-ben-gai-nian" class="header-anchor"></a></h3><ul><li>dataset : dataset 是一系列数据的集合，它的展现形式可以是Strings,  integers 甚至是存放在关系型数据库中的数据 </li></ul><h3><span id="chuang-jian-rdd">创建RDD</span><a href="#chuang-jian-rdd" class="header-anchor"></a></h3><blockquote><p>创建RDD 有两种方式，<em>Parallelized Collections</em> 和 <em>External Datasets</em></p></blockquote><p>在JAVA maven 项目中， 添加依赖即可引用spark 相关类</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.spark<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spark-core_2.12<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.4.3<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure><p>在maven 项目一般使用方式如下</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.lcc.spark;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.SparkConf;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.JavaSparkContext;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">HelloWordSpark</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        String appName = "HellowWorld"; #项目名称</span><br><span class="line">        String master = "local"; # 运行模式</span><br><span class="line">        SparkConf conf = <span class="keyword">new</span> SparkConf().setAppName(appName).setMaster(master);</span><br><span class="line">        JavaSparkContext sc = <span class="keyword">new</span> JavaSparkContext(conf);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4><span id="parallelized-collections">Parallelized Collections</span><a href="#parallelized-collections" class="header-anchor"></a></h4><p>将现有的数据直接转换成RDD , 但是一般使用场景不太会用到</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">List&lt;Integer&gt; data = Arrays.asList(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>);</span><br><span class="line">JavaRDD&lt;Integer&gt; distData = sc.parallelize(data);</span><br></pre></td></tr></table></figure><h4><span id="external-datasets-cong-wai-bu-wen-jian-du-qu">External Datasets 从外部文件读取</span><a href="#external-datasets-cong-wai-bu-wen-jian-du-qu" class="header-anchor"></a></h4><p>一般情况下需要分析的Spark文件都比较大，存放在HDFS或者HBASe 等外部文件，比较适用实际使用场景<br>使用方法</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">JavaRDD&lt;String&gt; distFile = sc.textFile(<span class="string">"data.txt"</span>);</span><br></pre></td></tr></table></figure><h2><span id="rdd-cao-zuo">RDD 操作</span><a href="#rdd-cao-zuo" class="header-anchor"></a></h2><blockquote><p>RDD 主要包含transformaitons 和 actions 两个阶段的操作</p></blockquote><h3><span id="transformation">Transformation</span><a href="#transformation" class="header-anchor"></a></h3><blockquote><p>Transformation 主要目的是转换数据， 从已经存在的数据集上新创建一个</p></blockquote><p>transformations 是RDD上一系列操作，可以返回一个<strong>新的RDD</strong>，Spark 中所有的transformation都是采用的LAZY模式，他不会立马计算出结果，直到有操作需要计算新的RDD时才会进行计算，默认情况下，每一个transformed RDD 每次调用可能出现重复计算，可以通过<em>persist</em> RDD 到内存。</p><blockquote><p>注意 ： transformations 会返回一个新的Rdd</p></blockquote><p>最常见的transformation 操作时<strong>过滤filter</strong> 和 <strong>映射map</strong>操作<br>代码案例–统计文本中每个单词出现的次数</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.lcc.spark;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.SparkConf;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.JavaRDD;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.JavaSparkContext;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.function.FlatMapFunction;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.Arrays;</span><br><span class="line"><span class="keyword">import</span> java.util.Iterator;</span><br><span class="line"><span class="keyword">import</span> java.util.Map;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Hello world!</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">WordsCount</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        String appName = <span class="string">"HellowWorld"</span>;</span><br><span class="line">        String master = <span class="string">"local"</span>;</span><br><span class="line">        SparkConf conf = <span class="keyword">new</span> SparkConf().setAppName(appName).setMaster(master);</span><br><span class="line">        JavaSparkContext sparkContext = <span class="keyword">new</span> JavaSparkContext(conf);</span><br><span class="line">        JavaRDD&lt;String&gt; stringJavaRDD = sparkContext.textFile(<span class="string">"src/main/resources/input/word_count.text"</span>);</span><br><span class="line">        JavaRDD&lt;String&gt; objectJavaRDD = stringJavaRDD.flatMap(<span class="keyword">new</span> FlatMapFunction&lt;String, String&gt;() &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> Iterator&lt;String&gt; <span class="title">call</span><span class="params">(String s)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                <span class="keyword">return</span> Arrays.asList(s.split(<span class="string">" "</span>)).iterator();</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line">        Map&lt;String, Long&gt; stringLongMap = objectJavaRDD.countByValue();</span><br><span class="line">        <span class="keyword">for</span> (Map.Entry&lt;String, Long&gt; entry : stringLongMap.entrySet()) &#123;</span><br><span class="line">            System.out.println(<span class="string">"Key = "</span> + entry.getKey() + <span class="string">", Value = "</span> + entry.getValue());</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4><span id="flatmap-he-map">flatMap 和 Map</span><a href="#flatmap-he-map" class="header-anchor"></a></h4><p><img src="https://i.loli.net/2019/07/05/5d1eeb31393e167850.png" alt="1560780323084.png"><br><strong>Map</strong>适用场景为<strong>1对1</strong>映射关系<br><img src="https://i.loli.net/2019/07/05/5d1eeb307fa8d34013.png" alt="1560780356961.png"><br><strong>FlatMap</strong>适用场景是<strong>1对多</strong>映射关系<br><img src="https://i.loli.net/2019/07/05/5d1eeb30ccd7c49936.png" alt="1560780392131.png"></p><h4><span id="sample-cao-zuo">sample 操作</span><a href="#sample-cao-zuo" class="header-anchor"></a></h4><p>sample 操作会返回一个随机的rdd， 常用语获取随机数据</p><h4><span id="distinct-cao-zuo">distinct 操作</span><a href="#distinct-cao-zuo" class="header-anchor"></a></h4><p>返回唯一的distinct操作</p><h4><span id="union-he-bing-cao-zuo">union 合并操作</span><a href="#union-he-bing-cao-zuo" class="header-anchor"></a></h4><p>union 操作实质是求并集合， union = A U B</p><h4><span id="intersection-jiao-ji">intersection 交集</span><a href="#intersection-jiao-ji" class="header-anchor"></a></h4><p>intersection operation 操作实质是求交集 intersection = A∩B</p><blockquote><p>注意 intersection 操作时非常消耗资源的，因为需要把所有的分区都遍历才能获取到交集<br>所获取到的交集结果会去重处理</p></blockquote><h4><span id="substract-zi-ji">substract 子集</span><a href="#substract-zi-ji" class="header-anchor"></a></h4><blockquote><p>注意 intersection 操作时非常消耗资源的，因为需要把所有的分区都遍历才能获取到子集</p></blockquote><h4><span id="cartesian-di-qia-er-ji">cartesian 笛卡尔积</span><a href="#cartesian-di-qia-er-ji" class="header-anchor"></a></h4><p>返回RDD A 和 RDD B 笛卡尔积</p><h3><span id="actions">Actions</span><a href="#actions" class="header-anchor"></a></h3><blockquote><p>actions是将最终值返回给驱动程序或将数据持久化到外部存储系统的操作</p></blockquote><h4><span id="collect-cao-zuo">collect 操作</span><a href="#collect-cao-zuo" class="header-anchor"></a></h4><h4><span id="count-he-countbyvalue">count 和 countByValue</span><a href="#count-he-countbyvalue" class="header-anchor"></a></h4><ul><li><strong>count</strong> 用于统计有多行数据在当前rdd当中，count 会返回有多少元素</li><li><strong>countByValue</strong> 会在当前RDD下，按照唯一值进行计数， 并返回一个map(这个很类似mysql的count函数)</li></ul><h4><span id="take-cao-zuo">take 操作</span><a href="#take-cao-zuo" class="header-anchor"></a></h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">words = wordRdd.tabke(<span class="number">3</span>)</span><br></pre></td></tr></table></figure><ul><li>take action 从当前RDD 当中获取N个元素</li><li>take action 常用语单元测试和debug</li></ul><h4><span id="saveastextfile-cao-zuo">saveAsTextFile 操作</span><a href="#saveastextfile-cao-zuo" class="header-anchor"></a></h4><p>saveAsTextFile操作主要是将数据存放到硬盘中，这个查询之前的即可</p><h4><span id="reduce-cao-zuo">reduce 操作</span><a href="#reduce-cao-zuo" class="header-anchor"></a></h4><p>reduce 操作与mapReduce 中思想一样</p><p><img src="https://i.loli.net/2019/07/05/5d1eeb311fd2313743.png" alt="1560780429240.png"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;div class=&quot;toc&quot;&gt;

&lt;!-- toc --&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;#rdd-jian-jie&quot;&gt;RDD 简介&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;#ji-ben-gai-nian&quot;&gt;基本概念&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#
      
    
    </summary>
    
    
      <category term="Spark" scheme="http://yoursite.com/tags/Spark/"/>
    
      <category term="大数据" scheme="http://yoursite.com/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
  </entry>
  
  <entry>
    <title>Spark学习（1）安装Spark</title>
    <link href="http://yoursite.com/2019/06/16/sparkInstall/"/>
    <id>http://yoursite.com/2019/06/16/sparkInstall/</id>
    <published>2019-06-16T08:18:16.000Z</published>
    <updated>2019-06-16T08:56:41.341Z</updated>
    
    <content type="html"><![CDATA[<div class="toc"><!-- toc --><ul><li><a href="#spark-jian-jie">Spark 简介</a></li><li><a href="#spark-an-zhuang">Spark 安装</a><ul><li><a href="#cai-yong-an-zhuang-bao-de-fang-shi-an-zhuang">采用安装包的方式安装</a><ul><li><a href="#linux-shang-an-zhuang-spark">LINUX 上安装Spark</a></li><li><a href="#windows-an-zhuang-spark">Windows 安装Spark</a></li><li><a href="#cai-yong-yuan-ma-fang-shi-an-zhuang">采用源码方式安装</a></li></ul></li></ul></li></ul><!-- tocstop --></div><h2><span id="spark-jian-jie">Spark 简介</span><a href="#spark-jian-jie" class="header-anchor"></a></h2><blockquote><p>Apache Spark™ is a unified analytics engine for large-scale data processing.<br>若需要查询更多信息，访问Spark <a href="https://spark.apache.org/" target="_blank" rel="noopener">官方网站</a><br>若需要查询Spark源码，访问Spark <a href="https://github.com/apache/spark" target="_blank" rel="noopener">Git Hub</a></p></blockquote><p>Spark 是一个快速且通用的集群计算平台</p><ul><li>Spark 是快速的 Spark 扩充了流行MapReduce 计算模型， 基于内存计算</li><li>Spark 通用的， 容纳了其它分布式系统拥有的功能，批处理，迭代式计算，交互式查询和流处理，降低集群维护成本</li><li>高度开放的，提供了Python, Java等高级API</li></ul><h2><span id="spark-an-zhuang">Spark 安装</span><a href="#spark-an-zhuang" class="header-anchor"></a></h2><p>Spark 安装方式提供两种安装模式，<strong>编译源码安装</strong> 和 <strong>安装包方式安装</strong>示例安装系统为windows，Linux , 参照对应操作系统安装即可</p><p>系统环境要求</p><ol><li><strong>安装JAVA JDK1.8</strong><br>为什么需要安装JAVA JDK1.8 ？</li></ol><ul><li>Spark 是采用Scala 语言编写，Scala语言是运行在JVM上的</li><li>Python API（PySpark） 与Spark进行交互, PySpark 是在<em>Spark‘s JAVA API</em>之上的封装,JAVA API 也是需要JDK</li><li>JAVA JDK 版本要求是1.8</li></ul><h3><span id="cai-yong-an-zhuang-bao-de-fang-shi-an-zhuang">采用安装包的方式安装</span><a href="#cai-yong-an-zhuang-bao-de-fang-shi-an-zhuang" class="header-anchor"></a></h3><ol><li>打开Spark <a href="https://spark.apache.org/" target="_blank" rel="noopener">官方网站</a>，点击DownLoad, 这里已经Spark 已经集成成功了hadoop组件，可以直接使用<br><img src="https://i.loli.net/2019/06/16/5d05fd9d8fa2f41078.png" alt="1.png"></li></ol><h4><span id="linux-shang-an-zhuang-spark">LINUX 上安装Spark</span><a href="#linux-shang-an-zhuang-spark" class="header-anchor"></a></h4><figure class="highlight powershell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[root@Amy ~]<span class="comment"># mkdir download</span></span><br><span class="line">[root@Amy ~]<span class="comment"># cd download/</span></span><br><span class="line"><span class="comment">#wget &lt;spark_url&gt; //使用wget 命令下载spark</span></span><br><span class="line">[root@Amy download]<span class="comment">#</span></span><br><span class="line">wget http://mirror.bit.edu.cn/apache/spark/spark-<span class="number">2.4</span>.<span class="number">3</span>/spark-<span class="number">2.4</span>.<span class="number">3</span>-bin-hadoop2.<span class="number">7</span>.tgz</span><br><span class="line">mkdir /opt/apache-spark <span class="comment"># spakr 解压目录</span></span><br><span class="line">[root@Amy download]<span class="comment"># tar xvzf spark-2.4.3-bin-hadoop2.7.tgz -C /opt/apache-spark/</span></span><br></pre></td></tr></table></figure><ol start="3"><li>配置环境变量<figure class="highlight powershell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">[root@Amy download]<span class="comment"># cd /opt/apache-spark/spark-2.4.3-bin-hadoop2.7/</span></span><br><span class="line">[root@Amy spark-<span class="number">2.4</span>.<span class="number">3</span>-bin-hadoop2.<span class="number">7</span>]<span class="comment"># vim ~/.bashrc</span></span><br><span class="line"><span class="comment">#将Spark 环境配置加入</span></span><br><span class="line"><span class="comment">#export SPARK_HOME=/opt/apache-spark/spark-2.4.3-bin-hadoop2.7</span></span><br><span class="line"><span class="comment">#export PATH=$PATH:$SPARK_HOME/bin</span></span><br><span class="line"><span class="comment">#保存文件并退出</span></span><br><span class="line">[root@Amy spark-<span class="number">2.4</span>.<span class="number">3</span>-bin-hadoop2.<span class="number">7</span>]<span class="comment"># source ~/.bashrc # 使得环境变量生效</span></span><br><span class="line">[root@Amy spark-<span class="number">2.4</span>.<span class="number">3</span>-bin-hadoop2.<span class="number">7</span>]<span class="comment"># pyspark # 执行pySpark 命令</span></span><br></pre></td></tr></table></figure></li></ol><p>pySpark 命令正确执行结果如下<br><img src="https://i.loli.net/2019/06/16/5d05fd9d6762132822.png" alt="2.png"></p><h4><span id="windows-an-zhuang-spark">Windows 安装Spark</span><a href="#windows-an-zhuang-spark" class="header-anchor"></a></h4><ol><li><p>通过浏览下载刚刚选择的Spark 版本</p></li><li><p>将Spark解压到需要保存的文件目录<br>这里以D盘根目录为例<br><img src="https://i.loli.net/2019/06/16/5d05fd9db812640075.png" alt="3.png"></p><blockquote><p><strong>注意</strong> Spark 的安装与其它操作系统没有区别，但是由于Windows操作系统采用的是NTFS格式，hadoop 不能完全兼容所需，所以需要稍微进行处理</p></blockquote></li><li><p>下载<a href="https://github.com/jleetutorial/sparkTutorial/blob/winutils/winutils.exe" target="_blank" rel="noopener">工具</a><br>保存路径到<em>D:\hadoop\bin</em>（这里需要注意，后续配置环境变量需要）</p></li><li><p>配置环境变量</p></li></ol><ul><li>配置<em>HADOOP_HOME</em> 为 <em>D:\hadoop</em>（与上一步hadoop工具一致）</li><li>配置<em>SPARK_HOME *为</em>D:\spark-2.4.3-bin-hadoop2.7*（Spark 保存路径）</li><li>添加Hadoop 和Spark 到Path 中</li></ul><ol start="4"><li>校验安装是否成功</li></ol><ul><li>新创建一个文件目录<em>D:\tmp\hive</em>，使用工具修改文件夹格式<figure class="highlight crmsh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">C:\Users\Administrator&gt;winutils chmod <span class="number">777</span> D:\tmp\hive</span><br><span class="line">C:\Users\Administrator&gt;pyspark</span><br></pre></td></tr></table></figure></li></ul><p><img src="https://i.loli.net/2019/06/16/5d05fd9d7e00675647.png" alt="4.png"></p><h4><span id="cai-yong-yuan-ma-fang-shi-an-zhuang">采用源码方式安装</span><a href="#cai-yong-yuan-ma-fang-shi-an-zhuang" class="header-anchor"></a></h4><p>详细源码安装<a href="http://spark.apache.org/docs/latest/building-spark.html" target="_blank" rel="noopener">官方文档</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;div class=&quot;toc&quot;&gt;

&lt;!-- toc --&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;#spark-jian-jie&quot;&gt;Spark 简介&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#spark-an-zhuang&quot;&gt;Spark 安装&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a
      
    
    </summary>
    
    
      <category term="Spark" scheme="http://yoursite.com/tags/Spark/"/>
    
      <category term="大数据" scheme="http://yoursite.com/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
  </entry>
  
  <entry>
    <title>制作HEXO Docker 镜像</title>
    <link href="http://yoursite.com/2019/06/08/build-hexo-with-docker/"/>
    <id>http://yoursite.com/2019/06/08/build-hexo-with-docker/</id>
    <published>2019-06-08T03:15:56.000Z</published>
    <updated>2019-06-08T07:31:03.975Z</updated>
    
    <content type="html"><![CDATA[<div class="toc"><!-- toc --><ul><li><a href="#zhi-zuo-hexo-docker-jing-xiang">制作HEXO Docker镜像</a><ul><li><a href="#qi-dong-docker-rong-qi">启动docker 容器</a></li><li><a href="#pei-zhi-ubuntu-apt-get-shu-ju-yuan">配置ubuntu apt-get数据源</a></li><li><a href="#ubuntu-an-zhuang-git">ubuntu安装git</a></li><li><a href="#an-zhuang-node-js">安装node js</a><ul><li><a href="#huo-qu-node-js-ban-ben">获取node js 版本</a></li></ul></li><li><a href="#an-zhuang-hexo">安装hexo</a></li><li><a href="#jiang-rong-qi-da-cheng-jing-xiang">将容器打成镜像</a></li><li><a href="#pei-zhi-tu-pian-cha-jian">配置图片插件</a></li><li><a href="#can-kao-wen-dang">参考文档</a></li></ul></li></ul><!-- tocstop --></div><h2><span id="zhi-zuo-hexo-docker-jing-xiang">制作HEXO Docker镜像</span><a href="#zhi-zuo-hexo-docker-jing-xiang" class="header-anchor"></a></h2><p>Hexo是一个基于node.js的静态博客生成系统，它使用markdown语法来写作，同时支持丰富的自定义标签系统。但是对于后端开发者来讲， 本身对于Nodejs 不太熟悉，如果更换电脑又得捣腾一遍环境，所以笔者直接将hexo环境打成docker镜像，以便日后更好的使用</p><p>镜像路径 : <a href="https://hub.docker.com/r/changchenglei/hexo" target="_blank" rel="noopener">https://hub.docker.com/r/changchenglei/hexo</a><br>拉取镜像指令:  </p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker pull changchenglei/hexo</span><br></pre></td></tr></table></figure><p>本文采用ubuntu做为操作系统，进行容器化搭建.</p><h3><span id="qi-dong-docker-rong-qi">启动docker 容器</span><a href="#qi-dong-docker-rong-qi" class="header-anchor"></a></h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">docker pull ubuntu <span class="comment"># 拉取最新的ubuntu镜像</span></span><br><span class="line">docker run --name hexo_blog -d -i -t ubuntu <span class="comment">#启动容器</span></span><br><span class="line">docker <span class="built_in">exec</span> -it hexo_blog <span class="string">"/bin/bash"</span> <span class="comment">#进入容器进行安装</span></span><br></pre></td></tr></table></figure><h3><span id="pei-zhi-ubuntu-apt-get-shu-ju-yuan">配置ubuntu apt-get数据源</span><a href="#pei-zhi-ubuntu-apt-get-shu-ju-yuan" class="header-anchor"></a></h3><blockquote><p>当网络缓慢可以设置，若网络良好可以直接忽略<br>配置apt-get 数据源主要是为了加快下载速度，若网络速度已经很快可以不用修改</p></blockquote><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">docker run --name hexo_blog -d -i -t ubuntu</span><br><span class="line">sudo cp /etc/apt/sources.list /etc/apt/sources_init.list</span><br><span class="line">apt-get update</span><br><span class="line">apt-get upgrade</span><br><span class="line">apt-get install sudo</span><br><span class="line">apt-get install vim</span><br><span class="line">sudo vi /etc/apt/sources.list <span class="comment">#配置阿里数据源</span></span><br><span class="line">sudo apt-get update</span><br><span class="line">sudo apt-get -f install</span><br></pre></td></tr></table></figure><p>配置apt-get 阿里数据源， 不要删除原有的，将阿里数据源添加到最前面</p><figure class="highlight tex"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">deb http://mirrors.aliyun.com/ubuntu/ xenial main</span><br><span class="line">deb-src http://mirrors.aliyun.com/ubuntu/ xenial main</span><br><span class="line"></span><br><span class="line">deb http://mirrors.aliyun.com/ubuntu/ xenial-updates main</span><br><span class="line">deb-src http://mirrors.aliyun.com/ubuntu/ xenial-updates main</span><br><span class="line"></span><br><span class="line">deb http://mirrors.aliyun.com/ubuntu/ xenial universe</span><br><span class="line">deb-src http://mirrors.aliyun.com/ubuntu/ xenial universe</span><br><span class="line">deb http://mirrors.aliyun.com/ubuntu/ xenial-updates universe</span><br><span class="line">deb-src http://mirrors.aliyun.com/ubuntu/ xenial-updates universe</span><br><span class="line"></span><br><span class="line">deb http://mirrors.aliyun.com/ubuntu/ xenial-security main</span><br><span class="line">deb-src http://mirrors.aliyun.com/ubuntu/ xenial-security main</span><br><span class="line">deb http://mirrors.aliyun.com/ubuntu/ xenial-security universe</span><br><span class="line">deb-src http://mirrors.aliyun.com/ubuntu/ xenial-security universe</span><br></pre></td></tr></table></figure><h3><span id="ubuntu-an-zhuang-git">ubuntu安装git</span><a href="#ubuntu-an-zhuang-git" class="header-anchor"></a></h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">apt-get install git <span class="comment">#这里会自动安装包</span></span><br><span class="line">git --version</span><br></pre></td></tr></table></figure><h3><span id="an-zhuang-node-js">安装node js</span><a href="#an-zhuang-node-js" class="header-anchor"></a></h3><h4><span id="huo-qu-node-js-ban-ben">获取node js 版本</span><a href="#huo-qu-node-js-ban-ben" class="header-anchor"></a></h4><ol><li>打开<a href="https://nodejs.org/en/download/" target="_blank" rel="noopener">官网下载</a></li><li>找到需要下载的版本， 右键制下载链接， 这里以node-v12.3.1 为例<br><img src="https://i.loli.net/2019/06/08/5cfb56abc14f540413.png" alt="nodeJs.png"><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-get install wget</span><br><span class="line">wget https://nodejs.org/dist/v12.3.1/node-v12.3.1-linux-x64.tar.xz  </span><br><span class="line">tar -xJf node-v12.3.1-linux-x64.tar.xz -C /opt</span><br><span class="line">sudo ln -s /opt/node-v12.3.1-linux-x64/bin/node /usr/<span class="built_in">local</span>/bin/node <span class="comment">#建立软连接</span></span><br><span class="line">sudo ln -s /opt/node-v12.3.1-linux-x64/bin/npm /usr/<span class="built_in">local</span>/bin/npm <span class="comment">#建立软连接</span></span><br><span class="line">npm -v</span><br><span class="line">node -v</span><br><span class="line">sudo npm config <span class="built_in">set</span> registry https://registry.npm.taobao.org <span class="comment">#设置阿里npm数据源</span></span><br><span class="line"><span class="built_in">source</span> ~/.bashrc <span class="comment">#设置数据源立马生效</span></span><br></pre></td></tr></table></figure></li></ol><h3><span id="an-zhuang-hexo">安装hexo</span><a href="#an-zhuang-hexo" class="header-anchor"></a></h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">npm install -g hexo-cli</span><br><span class="line">sudo ln -s /opt/node-v12.3.1-linux-x64/bin/hexo /usr/<span class="built_in">local</span>/bin/hexo <span class="comment">#建立hexo软连接</span></span><br></pre></td></tr></table></figure><p>默认hexo 工作间是在/home/hexo 目录下，有需求可以自行更改</p><h3><span id="jiang-rong-qi-da-cheng-jing-xiang">将容器打成镜像</span><a href="#jiang-rong-qi-da-cheng-jing-xiang" class="header-anchor"></a></h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker commit -m <span class="string">"Add original docker hexo enviroment, inclue node js(v12.3.1) , git(2.17.1), hexo "</span> -a <span class="string">"LeiChangCheng"</span> 273901f62fb9 docker.io/changchenglei/hexo</span><br></pre></td></tr></table></figure><h3><span id="pei-zhi-tu-pian-cha-jian">配置图片插件</span><a href="#pei-zhi-tu-pian-cha-jian" class="header-anchor"></a></h3><ol><li>把主页配置文件_config.yml 里的post_asset_folder:这个选项设置为true</li><li>在根目录下输入npm install –save hexo-asset-image，这是下载安装一个可以上传本地图片的插件<br>再运行hexo new “xxxx”来生成md博文时，/source/_posts文件夹内除了xxxx.md文件还有一个同名的文件夹<br>最后在xxxx.md中想引入图片时，先把图片复制到xxxx这个文件夹中，然后只需要在xxxx.md中按照markdown的格式引入图片即可<figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">![<span class="string">Alt text</span>](<span class="link">https://i.loli.net/2019/06/08/5cfb56abc14f540413.png</span>) // 互联网查找图片</span><br><span class="line">![<span class="string">Alt text</span>](<span class="link">./nodejs.png</span>) //本地查找图片</span><br></pre></td></tr></table></figure></li></ol><h3><span id="can-kao-wen-dang">参考文档</span><a href="#can-kao-wen-dang" class="header-anchor"></a></h3><p>[1] <a href="https://hexo.io/zh-cn/docs/" target="_blank" rel="noopener">hexo配置</a><br>[2] <a href="https://hexo.io/zh-cn/docs/" target="_blank" rel="noopener">hexo建站</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;div class=&quot;toc&quot;&gt;

&lt;!-- toc --&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;#zhi-zuo-hexo-docker-jing-xiang&quot;&gt;制作HEXO Docker镜像&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;#qi-dong-docker-ron
      
    
    </summary>
    
    
      <category term="hexo" scheme="http://yoursite.com/tags/hexo/"/>
    
      <category term="docker" scheme="http://yoursite.com/tags/docker/"/>
    
  </entry>
  
</feed>
