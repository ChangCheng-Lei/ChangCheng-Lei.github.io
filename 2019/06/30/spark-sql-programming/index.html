<!DOCTYPE html><html lang="zh-cn"><head><meta name="generator" content="Hexo 3.9.0"><meta charset="utf-8"><title>Spark学习（三）Spark-SQL详解 | ChangCheng Blog</title><meta http-equiv="X-UA-Compatible" content="IE=Edge,chrome=1"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=1,user-scalable=no"><meta name="format-detection" content="telephone=no"><meta name="author" content="[object Object]"><meta name="designer" content="minfive"><meta name="keywords" content="HEXO,SpringCLoud,MySQL,JAVA,Docker,微服务"><meta name="description" content="日常学习与兴趣交流的个人博客"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-status-bar-style" content="black"><meta name="format-detection" content="telephone=yes"><meta name="mobile-web-app-capable" content="yes"><meta name="robots" content="all"><link rel="canonical" href="http://yoursite.com/2019/06/30/spark-sql-programming/index.html"><link rel="icon" type="image/png" href="https://timgsa.baidu.com/timg?image&quality=80&size=b9999_10000&sec=1559928506945&di=406e9b9d2e125205da3c71fdfe56aedc&imgtype=0&src=http%3A%2F%2Fp5.so.qhimgs1.com%2Ft01031f510e7406ba8b.jpg" sizes="32x32"><link rel="stylesheet" href="/scss/base/index.css"><link rel="alternate" href="/atom.xml" title="ChangCheng"><link rel="stylesheet" href="/scss/views/page/post.css"></head><body ontouchstart><div id="page-loading" class="page page-loading" style="background-image:url(https://blog.static.minfive.com/other/loader.gif)"></div><div id="page" class="page js-hidden"><header class="page__small-header page__header--small"><nav class="page__navbar"><div class="page__container navbar-container"><a class="page__logo" href="/" title="ChangCheng" alt="ChangCheng"><img src="https://timgsa.baidu.com/timg?image&quality=80&size=b9999_10000&sec=1559928575075&di=8c27770cd8f303fae3c33eb59b8c5519&imgtype=0&src=http%3A%2F%2Fimg.ph.126.net%2Fju7M-CM1VEWcZGXPJlgmDA%3D%3D%2F3275242829007105662.png" alt="ChangCheng"></a><nav class="page__nav"><ul class="nav__list clearfix"><li class="nav__item"><a href="/" alt="首页" title="首页">首页</a></li><li class="nav__item"><a href="/archives" alt="归档" title="归档">归档</a></li><li class="nav__item"><a href="/about" alt="关于" title="关于">关于</a></li></ul></nav><button class="page__menu-btn" type="button"><i class="iconfont icon-menu"></i></button></div></nav></header><main class="page__container page__main"><div class="page__content"><article class="page__post"><div class="post__cover"><img src="https://i.loli.net/2019/06/16/5d05fd9da592065728.jpg" alt="Spark学习（三）Spark-SQL详解"></div><header class="post__info"><h1 class="post__title">Spark学习（三）Spark-SQL详解</h1><div class="post__mark"><div class="mark__block"><i class="mark__icon iconfont icon-write"></i><ul class="mark__list clearfix"><li class="mark__item"><a href="https://github.com/ChangCheng-Lei">雷昌诚</a></li></ul></div><div class="mark__block"><i class="mark__icon iconfont icon-time"></i><ul class="mark__list clearfix"><li class="mark__item"><span>2019-06-30</span></li></ul></div><div class="mark__block"><i class="mark__icon iconfont icon-tab"></i><ul class="mark__list clearfix"><li class="mark__item"><a href="/tags/Spark/">Spark</a></li><li class="mark__item"><a href="/tags/大数据/">大数据</a></li></ul></div><div class="mark__block"><i class="mark__icon iconfont icon-eye"></i><ul class="mark__list clearfix"><li id="busuanzi_container_page_pv" class="mark__item"><span id="busuanzi_value_page_pv"></span>次</li></ul></div></div></header><div class="post__content"><div class="toc"><ul><li><a href="#spark-xue-xi-san-spark-sql-xiang-jie">Spark学习（三）Spark-SQL详解</a><ul><li><a href="#spark-sql-jian-jie">Spark SQL 简介</a><ul><li><a href="#spark-sql-jia-gou">Spark SQL 架构</a></li></ul></li><li><a href="#spark-sql-ji-ben-shi-yong">Spark SQL 基本使用</a><ul><li><a href="#sparksession-zhu-ru-kou">SparkSession主入口</a></li><li><a href="#chuang-jian-dataframes">创建DataFrames</a></li><li><a href="#dataframe-ji-ben-cao-zuo">DataFrame 基本操作</a></li><li><a href="#zhi-xing-dong-tai-bian-cheng-cha-xun-sql">执行动态编程查询SQL</a></li><li><a href="#quan-ju-lin-shi-shi-tu">全局临时视图</a></li><li><a href="#chuang-jian-shu-ju-ji">创建数据集</a></li><li><a href="#jiang-rdd-zhuan-huan-cheng-dataset">将RDD转换成Dataset</a><ul><li><a href="#shi-yong-fan-she-tui-duan-mo-shi">使用反射推断模式</a></li><li><a href="#yi-bian-cheng-fang-shi-zhi-ding-mo-shi">以编程方式指定模式</a></li></ul></li></ul></li><li><a href="#datasource-shu-ju-yuan">DataSource数据源</a></li></ul></li></ul></div><h1><span id="spark-xue-xi-san-spark-sql-xiang-jie">Spark学习（三）Spark-SQL详解</span><a href="#spark-xue-xi-san-spark-sql-xiang-jie" class="header-anchor"></a></h1><p>@(BigData)[Spark, 大数据]</p><h2><span id="spark-sql-jian-jie">Spark SQL 简介</span><a href="#spark-sql-jian-jie" class="header-anchor"></a></h2><p>Spark SQL是一个用于结构化数据处理的Spark模块。与基本的Spark RDD API不同，Spark SQL提供的接口为Spark提供了关于数据结构和正在执行的计算的更多信息。</p><p>本文所有源代码可以访问<a href="https://github.com/ChangCheng-Lei/Java-Spark-Tutorial" target="_blank" rel="noopener">GIT HUB</a>获取</p><ol><li><strong>Datasets</strong>是分布式的数据集合，Dataset 支持Scala 和 JAVA的访问</li><li><strong>DataFrame</strong> 是组织成命名列的数据集</li></ol><h3><span id="spark-sql-jia-gou">Spark SQL 架构</span><a href="#spark-sql-jia-gou" class="header-anchor"></a></h3><p><img src="https://i.loli.net/2019/07/05/5d1f20e27348d22212.png" alt="1561425410767.png"></p><h2><span id="spark-sql-ji-ben-shi-yong">Spark SQL 基本使用</span><a href="#spark-sql-ji-ben-shi-yong" class="header-anchor"></a></h2><h3><span id="sparksession-zhu-ru-kou">SparkSession主入口</span><a href="#sparksession-zhu-ru-kou" class="header-anchor"></a></h3><p>Spark SQL 程序的主入口是SparkSession, 所以其余操作是基于SparkSession之上的，创建SparkSession代码如下</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">SparkSession sparkSession = SparkSession.builder()</span><br><span class="line">                .appName(<span class="string">"Java Spark SQL basic example"</span>)</span><br><span class="line">                .master(<span class="string">"local"</span>)</span><br><span class="line">                .config(<span class="string">"spark.some.config.option"</span>, <span class="string">"some-value"</span>)</span><br><span class="line">                .getOrCreate();</span><br></pre></td></tr></table></figure><h3><span id="chuang-jian-dataframes">创建DataFrames</span><a href="#chuang-jian-dataframes" class="header-anchor"></a></h3><p>通过<em>SparkSession</em>, 程序可以通过已经存在的RDD, Hive table, Spark data sources 来创建DataFrame。<br>一下程序就是通过Json文件创建Dataset</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.sql.Dataset;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.Row;</span><br><span class="line"></span><br><span class="line">Dataset&lt;Row&gt; df = spark.read().json(<span class="string">"examples/src/main/resources/people.json"</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">// Displays the content of the DataFrame to stdout</span></span><br><span class="line">df.show();</span><br><span class="line"><span class="comment">// +----+-------+</span></span><br><span class="line"><span class="comment">// | age|   name|</span></span><br><span class="line"><span class="comment">// +----+-------+</span></span><br><span class="line"><span class="comment">// |null|Michael|</span></span><br><span class="line"><span class="comment">// |  30|   Andy|</span></span><br><span class="line"><span class="comment">// |  19| Justin|</span></span><br><span class="line"><span class="comment">// +----+-</span></span><br></pre></td></tr></table></figure><h3><span id="dataframe-ji-ben-cao-zuo">DataFrame 基本操作</span><a href="#dataframe-ji-ben-cao-zuo" class="header-anchor"></a></h3><p>DataFrame 基本操作类似于关系型数据库SQL操作,所有的操作可以查看<a href="http://spark.apache.org/docs/latest/api/java/org/apache/spark/sql/Dataset.html" target="_blank" rel="noopener">DataSet JAVA API</a></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// col("...") is preferable to df.col("...")</span></span><br><span class="line"><span class="keyword">import</span> <span class="keyword">static</span> org.apache.spark.sql.functions.col;</span><br><span class="line"></span><br><span class="line"><span class="comment">// Print the schema in a tree format</span></span><br><span class="line">df.printSchema();</span><br><span class="line"><span class="comment">// root</span></span><br><span class="line"><span class="comment">// |-- age: long (nullable = true)</span></span><br><span class="line"><span class="comment">// |-- name: string (nullable = true)</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// Select only the "name" column</span></span><br><span class="line">df.select(<span class="string">"name"</span>).show();</span><br><span class="line"><span class="comment">// +-------+</span></span><br><span class="line"><span class="comment">// |   name|</span></span><br><span class="line"><span class="comment">// +-------+</span></span><br><span class="line"><span class="comment">// |Michael|</span></span><br><span class="line"><span class="comment">// |   Andy|</span></span><br><span class="line"><span class="comment">// | Justin|</span></span><br><span class="line"><span class="comment">// +-------+</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// Select everybody, but increment the age by 1</span></span><br><span class="line">df.select(col(<span class="string">"name"</span>), col(<span class="string">"age"</span>).plus(<span class="number">1</span>)).show();</span><br><span class="line"><span class="comment">// +-------+---------+</span></span><br><span class="line"><span class="comment">// |   name|(age + 1)|</span></span><br><span class="line"><span class="comment">// +-------+---------+</span></span><br><span class="line"><span class="comment">// |Michael|     null|</span></span><br><span class="line"><span class="comment">// |   Andy|       31|</span></span><br><span class="line"><span class="comment">// | Justin|       20|</span></span><br><span class="line"><span class="comment">// +-------+---------+</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// Select people older than 21</span></span><br><span class="line">df.filter(col(<span class="string">"age"</span>).gt(<span class="number">21</span>)).show();</span><br><span class="line"><span class="comment">// +---+----+</span></span><br><span class="line"><span class="comment">// |age|name|</span></span><br><span class="line"><span class="comment">// +---+----+</span></span><br><span class="line"><span class="comment">// | 30|Andy|</span></span><br><span class="line"><span class="comment">// +---+----+</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// Count people by age</span></span><br><span class="line">df.groupBy(<span class="string">"age"</span>).count().show();</span><br><span class="line"><span class="comment">// +----+-----+</span></span><br><span class="line"><span class="comment">// | age|count|</span></span><br><span class="line"><span class="comment">// +----+-----+</span></span><br><span class="line"><span class="comment">// |  19|    1|</span></span><br><span class="line"><span class="comment">// |null|    1|</span></span><br><span class="line"><span class="comment">// |  30|    1|</span></span><br><span class="line"><span class="comment">// +----+-----+</span></span><br></pre></td></tr></table></figure><h3><span id="zhi-xing-dong-tai-bian-cheng-cha-xun-sql">执行动态编程查询SQL</span><a href="#zhi-xing-dong-tai-bian-cheng-cha-xun-sql" class="header-anchor"></a></h3><p><em>sql</em>方法基于SparkSession之上，可以定置化查询语句，其返回结果是<strong>Dataset<row></row></strong></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.sql.Dataset;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.Row;</span><br><span class="line"></span><br><span class="line"><span class="comment">// Register the DataFrame as a SQL temporary view</span></span><br><span class="line">df.createOrReplaceTempView(<span class="string">"people"</span>);</span><br><span class="line"></span><br><span class="line">Dataset&lt;Row&gt; sqlDF = spark.sql(<span class="string">"SELECT * FROM people"</span>);</span><br><span class="line">sqlDF.show();</span><br><span class="line"><span class="comment">// +----+-------+</span></span><br><span class="line"><span class="comment">// | age|   name|</span></span><br><span class="line"><span class="comment">// +----+-------+</span></span><br><span class="line"><span class="comment">// |null|Michael|</span></span><br><span class="line"><span class="comment">// |  30|   Andy|</span></span><br><span class="line"><span class="comment">// |  19| Justin|</span></span><br><span class="line"><span class="comment">// +----+-------+</span></span><br></pre></td></tr></table></figure><h3><span id="quan-ju-lin-shi-shi-tu">全局临时视图</span><a href="#quan-ju-lin-shi-shi-tu" class="header-anchor"></a></h3><p>Spark 提供视图功能，分为<strong>临时视图</strong>和<strong>全局临时视图</strong>，<em>临时视图</em>是随着创建Session的终止而消亡，如果需要创建一个临时视图在所有session当中共享，直到Spark服务终止才会消亡，则可以创建一个<em>全局临时视图</em>，<em>全局临时视图</em>会将数据保存到<em>global_temp</em>数据库中，使用时候必须显示的指定数据库，例如<br>SELECT * FROM global_temp.view1<br>示例代码：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Register the DataFrame as a global temporary view</span></span><br><span class="line">df.createGlobalTempView(<span class="string">"people"</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">// Global temporary view is tied to a system preserved database `global_temp`</span></span><br><span class="line">spark.sql(<span class="string">"SELECT * FROM global_temp.people"</span>).show();</span><br><span class="line"><span class="comment">// +----+-------+</span></span><br><span class="line"><span class="comment">// | age|   name|</span></span><br><span class="line"><span class="comment">// +----+-------+</span></span><br><span class="line"><span class="comment">// |null|Michael|</span></span><br><span class="line"><span class="comment">// |  30|   Andy|</span></span><br><span class="line"><span class="comment">// |  19| Justin|</span></span><br><span class="line"><span class="comment">// +----+-------+</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// Global temporary view is cross-session</span></span><br><span class="line">spark.newSession().sql(<span class="string">"SELECT * FROM global_temp.people"</span>).show();</span><br><span class="line"><span class="comment">// +----+-------+</span></span><br><span class="line"><span class="comment">// | age|   name|</span></span><br><span class="line"><span class="comment">// +----+-------+</span></span><br><span class="line"><span class="comment">// |null|Michael|</span></span><br><span class="line"><span class="comment">// |  30|   Andy|</span></span><br><span class="line"><span class="comment">// |  19| Justin|</span></span><br><span class="line"><span class="comment">// +----+-------+</span></span><br></pre></td></tr></table></figure><h3><span id="chuang-jian-shu-ju-ji">创建数据集</span><a href="#chuang-jian-shu-ju-ji" class="header-anchor"></a></h3><p>数据集与RDDs类似，它们不是使用Java序列化或Kryo，而是使用专门的编码器序列化对象，以便通过网络进行处理或传输。虽然编码器和标准序列化都负责将对象转换为字节，但编码器是动态生成的代码，使用的格式允许Spark执行许多操作，比如过滤、排序和散列，而无需将字节反序列化回对象。</p><p>示例代码</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> java.util.Arrays;</span><br><span class="line"><span class="keyword">import</span> java.util.Collections;</span><br><span class="line"><span class="keyword">import</span> java.io.Serializable;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.function.MapFunction;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.Dataset;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.Row;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.Encoder;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.Encoders;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">Person</span> <span class="keyword">implements</span> <span class="title">Serializable</span> </span>&#123;</span><br><span class="line">  <span class="keyword">private</span> String name;</span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">int</span> age;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">public</span> String <span class="title">getName</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> name;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setName</span><span class="params">(String name)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">this</span>.name = name;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">getAge</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> age;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setAge</span><span class="params">(<span class="keyword">int</span> age)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">this</span>.age = age;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// Create an instance of a Bean class</span></span><br><span class="line">Person person = <span class="keyword">new</span> Person();</span><br><span class="line">person.setName(<span class="string">"Andy"</span>);</span><br><span class="line">person.setAge(<span class="number">32</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">// Encoders are created for Java beans</span></span><br><span class="line">Encoder&lt;Person&gt; personEncoder = Encoders.bean(Person.class);</span><br><span class="line">Dataset&lt;Person&gt; javaBeanDS = spark.createDataset(</span><br><span class="line">  Collections.singletonList(person),</span><br><span class="line">  personEncoder</span><br><span class="line">);</span><br><span class="line">javaBeanDS.show();</span><br><span class="line"><span class="comment">// +---+----+</span></span><br><span class="line"><span class="comment">// |age|name|</span></span><br><span class="line"><span class="comment">// +---+----+</span></span><br><span class="line"><span class="comment">// | 32|Andy|</span></span><br><span class="line"><span class="comment">// +---+----+</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// Encoders for most common types are provided in class Encoders</span></span><br><span class="line">Encoder&lt;Integer&gt; integerEncoder = Encoders.INT();</span><br><span class="line">Dataset&lt;Integer&gt; primitiveDS = spark.createDataset(Arrays.asList(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>), integerEncoder);</span><br><span class="line">Dataset&lt;Integer&gt; transformedDS = primitiveDS.map(</span><br><span class="line">    (MapFunction&lt;Integer, Integer&gt;) value -&gt; value + <span class="number">1</span>,</span><br><span class="line">    integerEncoder);</span><br><span class="line">transformedDS.collect(); <span class="comment">// Returns [2, 3, 4]</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// DataFrames can be converted to a Dataset by providing a class. Mapping based on name</span></span><br><span class="line">String path = <span class="string">"examples/src/main/resources/people.json"</span>;</span><br><span class="line">Dataset&lt;Person&gt; peopleDS = spark.read().json(path).as(personEncoder);</span><br><span class="line">peopleDS.show();</span><br><span class="line"><span class="comment">// +----+-------+</span></span><br><span class="line"><span class="comment">// | age|   name|</span></span><br><span class="line"><span class="comment">// +----+-------+</span></span><br><span class="line"><span class="comment">// |null|Michael|</span></span><br><span class="line"><span class="comment">// |  30|   Andy|</span></span><br><span class="line"><span class="comment">// |  19| Justin|</span></span><br><span class="line"><span class="comment">// +----+-------+</span></span><br></pre></td></tr></table></figure><h3><span id="jiang-rdd-zhuan-huan-cheng-dataset">将RDD转换成Dataset</span><a href="#jiang-rdd-zhuan-huan-cheng-dataset" class="header-anchor"></a></h3><p>Spark 支持两种方式将RDD转换成Dataset.</p><ol><li>第一种方法使用反射来推断包含特定对象类型的RDD的模式。这种基于反射的方法可以生成更简洁的代码，并且当您在编写Spark应用程序时已经知道模式时，这种方法可以很好地工作。</li><li>创建数据集的第二种方法是通过编程接口，该接口允许您构造模式，然后将其应用于现有的RDD。虽然这个方法更详细，但它允许您在列及其类型直到运行时才知道时构造数据集。</li></ol><h4><span id="shi-yong-fan-she-tui-duan-mo-shi">使用反射推断模式</span><a href="#shi-yong-fan-she-tui-duan-mo-shi" class="header-anchor"></a></h4><p>示例代码：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.JavaRDD;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.function.Function;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.function.MapFunction;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.Dataset;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.Row;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.Encoder;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.Encoders;</span><br><span class="line"></span><br><span class="line"><span class="comment">// Create an RDD of Person objects from a text file</span></span><br><span class="line">JavaRDD&lt;Person&gt; peopleRDD = spark.read()</span><br><span class="line">  .textFile(<span class="string">"examples/src/main/resources/people.txt"</span>)</span><br><span class="line">  .javaRDD()</span><br><span class="line">  .map(line -&gt; &#123;</span><br><span class="line">    String[] parts = line.split(<span class="string">","</span>);</span><br><span class="line">    Person person = <span class="keyword">new</span> Person();</span><br><span class="line">    person.setName(parts[<span class="number">0</span>]);</span><br><span class="line">    person.setAge(Integer.parseInt(parts[<span class="number">1</span>].trim()));</span><br><span class="line">    <span class="keyword">return</span> person;</span><br><span class="line">  &#125;);</span><br><span class="line"></span><br><span class="line"><span class="comment">// Apply a schema to an RDD of JavaBeans to get a DataFrame</span></span><br><span class="line">Dataset&lt;Row&gt; peopleDF = spark.createDataFrame(peopleRDD, Person.class);</span><br><span class="line"><span class="comment">// Register the DataFrame as a temporary view</span></span><br><span class="line">peopleDF.createOrReplaceTempView(<span class="string">"people"</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">// SQL statements can be run by using the sql methods provided by spark</span></span><br><span class="line">Dataset&lt;Row&gt; teenagersDF = spark.sql(<span class="string">"SELECT name FROM people WHERE age BETWEEN 13 AND 19"</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">// The columns of a row in the result can be accessed by field index</span></span><br><span class="line">Encoder&lt;String&gt; stringEncoder = Encoders.STRING();</span><br><span class="line">Dataset&lt;String&gt; teenagerNamesByIndexDF = teenagersDF.map(</span><br><span class="line">    (MapFunction&lt;Row, String&gt;) row -&gt; <span class="string">"Name: "</span> + row.getString(<span class="number">0</span>),</span><br><span class="line">    stringEncoder);</span><br><span class="line">teenagerNamesByIndexDF.show();</span><br><span class="line"><span class="comment">// +------------+</span></span><br><span class="line"><span class="comment">// |       value|</span></span><br><span class="line"><span class="comment">// +------------+</span></span><br><span class="line"><span class="comment">// |Name: Justin|</span></span><br><span class="line"><span class="comment">// +------------+</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// or by field name</span></span><br><span class="line">Dataset&lt;String&gt; teenagerNamesByFieldDF = teenagersDF.map(</span><br><span class="line">    (MapFunction&lt;Row, String&gt;) row -&gt; <span class="string">"Name: "</span> + row.&lt;String&gt;getAs(<span class="string">"name"</span>),</span><br><span class="line">    stringEncoder);</span><br><span class="line">teenagerNamesByFieldDF.show();</span><br><span class="line"><span class="comment">// +------------+</span></span><br><span class="line"><span class="comment">// |       value|</span></span><br><span class="line"><span class="comment">// +------------+</span></span><br><span class="line"><span class="comment">// |Name: Justin|</span></span><br><span class="line"><span class="comment">// +------------+</span></span><br></pre></td></tr></table></figure><h4><span id="yi-bian-cheng-fang-shi-zhi-ding-mo-shi">以编程方式指定模式</span><a href="#yi-bian-cheng-fang-shi-zhi-ding-mo-shi" class="header-anchor"></a></h4><p>示例代码</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line">mport java.util.ArrayList;</span><br><span class="line"><span class="keyword">import</span> java.util.List;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.JavaRDD;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.function.Function;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.Dataset;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.Row;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.types.DataTypes;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.types.StructField;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.types.StructType;</span><br><span class="line"></span><br><span class="line"><span class="comment">// Create an RDD</span></span><br><span class="line">JavaRDD&lt;String&gt; peopleRDD = spark.sparkContext()</span><br><span class="line">  .textFile(<span class="string">"examples/src/main/resources/people.txt"</span>, <span class="number">1</span>)</span><br><span class="line">  .toJavaRDD();</span><br><span class="line"></span><br><span class="line"><span class="comment">// The schema is encoded in a string</span></span><br><span class="line">String schemaString = <span class="string">"name age"</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">// Generate the schema based on the string of schema</span></span><br><span class="line">List&lt;StructField&gt; fields = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line"><span class="keyword">for</span> (String fieldName : schemaString.split(<span class="string">" "</span>)) &#123;</span><br><span class="line">StructField field = DataTypes.createStructField(fieldName, DataTypes.StringType, <span class="keyword">true</span>);</span><br><span class="line">  fields.add(field);</span><br><span class="line">&#125;</span><br><span class="line">StructType schema = DataTypes.createStructType(fields);</span><br><span class="line"></span><br><span class="line"><span class="comment">// Convert records of the RDD (people) to Rows</span></span><br><span class="line">JavaRDD&lt;Row&gt; rowRDD = peopleRDD.map((Function&lt;String, Row&gt;) record -&gt; &#123;</span><br><span class="line">  String[] attributes = record.split(<span class="string">","</span>);</span><br><span class="line">  <span class="keyword">return</span> RowFactory.create(attributes[<span class="number">0</span>], attributes[<span class="number">1</span>].trim());</span><br><span class="line">&#125;);</span><br><span class="line"></span><br><span class="line"><span class="comment">// Apply the schema to the RDD</span></span><br><span class="line">Dataset&lt;Row&gt; peopleDataFrame = spark.createDataFrame(rowRDD, schema);</span><br><span class="line"></span><br><span class="line"><span class="comment">// Creates a temporary view using the DataFrame</span></span><br><span class="line">peopleDataFrame.createOrReplaceTempView(<span class="string">"people"</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">// SQL can be run over a temporary view created using DataFrames</span></span><br><span class="line">Dataset&lt;Row&gt; results = spark.sql(<span class="string">"SELECT name FROM people"</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">// The results of SQL queries are DataFrames and support all the normal RDD operations</span></span><br><span class="line"><span class="comment">// The columns of a row in the result can be accessed by field index or by field name</span></span><br><span class="line">Dataset&lt;String&gt; namesDS = results.map(</span><br><span class="line">    (MapFunction&lt;Row, String&gt;) row -&gt; <span class="string">"Name: "</span> + row.getString(<span class="number">0</span>),</span><br><span class="line">    Encoders.STRING());</span><br><span class="line">namesDS.show();</span><br><span class="line"><span class="comment">// +-------------+</span></span><br><span class="line"><span class="comment">// |        value|</span></span><br><span class="line"><span class="comment">// +-------------+</span></span><br><span class="line"><span class="comment">// |Name: Michael|</span></span><br><span class="line"><span class="comment">// |   Name: Andy|</span></span><br><span class="line"><span class="comment">// | Name: Justin|</span></span><br><span class="line"><span class="comment">// +-------------+</span></span><br></pre></td></tr></table></figure><h2><span id="datasource-shu-ju-yuan">DataSource数据源</span><a href="#datasource-shu-ju-yuan" class="header-anchor"></a></h2><p>Spark SQL支持通过DataFrame interfac对各种数据源进行操作，现在支持<strong>ORC文件</strong>，<strong>JSON文件</strong>，<strong>HIVE table</strong>,<strong>JDBC</strong>,<strong>Avro文件</strong><br>详细文档参考<a href="http://spark.apache.org/docs/latest/sql-data-sources.html" target="_blank" rel="noopener">官方文档</a></p><div class="post-announce">感谢您的阅读，本文由 <a href="https://github.com/ChangCheng-Lei">雷昌诚</a> 原创提供。如若转载，请注明出处：雷昌诚（<a href="https://github.com/ChangCheng-Lei">https://github.com/ChangCheng-Lei</a>）</div><div class="post__prevs"><div class="post__prev"><a href="/2019/06/23/spark-rdd-programming/" title="Spark学习（二）RDD详解"><i class="iconfont icon-prev"></i>Spark学习（二）RDD详解</a></div><div class="post__prev post__prev--right"><a href="/2019/07/10/hexo-using-local-image/" title="HEXO 使用本地图片">HEXO 使用本地图片<i class="iconfont icon-next"></i></a></div></div></div></article></div><aside class="page__sidebar"><form id="page-search-from" class="page__search-from" action="/search/"><label class="search-form__item"><input class="input" type="text" name="search" placeholder="Search..."> <i class="iconfont icon-search"></i></label></form><div class="sidebar__block"><h3 class="block__title">简介</h3><p class="block__text">日常学习与兴趣交流的个人博客</p></div><div class="sidebar__block"><h3 class="block__title">文章分类</h3></div><div class="sidebar__block"><h3 class="block__title">最新文章</h3><ul class="block-list latest-post-list"><li class="latest-post-item"><a href="/2019/07/10/hexo-using-local-image/" title="HEXO 使用本地图片"><div class="item__cover"><img src="http://47.99.130.173:4000/2019/07/10/hexo-using-local-image/hexo-banner.jpg" alt="HEXO 使用本地图片"></div><div class="item__info"><h3 class="item__title">HEXO 使用本地图片</h3><span class="item__text">2019-07-10</span></div></a></li><li class="latest-post-item"><a href="/2019/06/30/spark-sql-programming/" title="Spark学习（三）Spark-SQL详解"><div class="item__cover"><img src="https://i.loli.net/2019/06/16/5d05fd9da592065728.jpg" alt="Spark学习（三）Spark-SQL详解"></div><div class="item__info"><h3 class="item__title">Spark学习（三）Spark-SQL详解</h3><span class="item__text">2019-06-30</span></div></a></li><li class="latest-post-item"><a href="/2019/06/23/spark-rdd-programming/" title="Spark学习（二）RDD详解"><div class="item__cover"><img src="https://i.loli.net/2019/06/16/5d05fd9da592065728.jpg" alt="Spark学习（二）RDD详解"></div><div class="item__info"><h3 class="item__title">Spark学习（二）RDD详解</h3><span class="item__text">2019-06-23</span></div></a></li><li class="latest-post-item"><a href="/2019/06/16/sparkInstall/" title="Spark学习（1）安装Spark"><div class="item__cover"><img src="https://i.loli.net/2019/06/16/5d05fd9da592065728.jpg" alt="Spark学习（1）安装Spark"></div><div class="item__info"><h3 class="item__title">Spark学习（1）安装Spark</h3><span class="item__text">2019-06-16</span></div></a></li></ul></div><div class="sidebar__block"><h3 class="block__title">文章标签</h3><ul class="block-list tag-list clearfix"><li class="tag-item"><a class="tag-link" href="/tags/Spark/">Spark</a></li><li class="tag-item"><a class="tag-link" href="/tags/docker/">docker</a></li><li class="tag-item"><a class="tag-link" href="/tags/hexo/">hexo</a></li><li class="tag-item"><a class="tag-link" href="/tags/大数据/">大数据</a></li></ul></div></aside></main><footer class="page__footer"><section class="footer__top"><div class="page__container footer__container"><div class="footer-top__item footer-top__item--2"><h3 class="item__title">关于</h3><div class="item__content"><p class="item__text">本站是基于 Hexo 搭建的静态资源博客，主要用于分享日常学习、生活及工作的一些心得总结，欢迎点击右下角订阅 rss。</p><ul class="footer__contact-info"><li class="contact-info__item"><i class="iconfont icon-address"></i> <span>ChengDu, SiChuan Province, China</span></li><li class="contact-info__item"><i class="iconfont icon-email2"></i> <span>leichangchengvip@163.com</span></li></ul></div></div></div></section><section class="footer__bottom"><div class="page__container footer__container"><p class="footer__copyright">© <a href="https://github.com/Mrminfive/hexo-theme-skapp" target="_blank">Skapp</a> 2017 powered by <a href="http://hexo.io/" target="_blank">Hexo</a>, made by <a href="https://github.com/Mrminfive" target="_blank">minfive</a>.</p><ul class="footer__social-network clearfix"></ul></div></section></footer><div id="back-top" class="back-top back-top--hidden js-hidden"><i class="iconfont icon-top"></i></div></div><script src="/js/common.js"></script><script src="/js/page/post.js"></script><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></body></html>