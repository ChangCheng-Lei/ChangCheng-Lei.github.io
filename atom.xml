<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>ChangCheng</title>
  
  <subtitle>ChangCheng Blog</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2019-07-05T10:13:47.486Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>[object Object]</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Spark学习（三）Spark-SQL详解</title>
    <link href="http://yoursite.com/2019/06/30/spark-sql-programming/"/>
    <id>http://yoursite.com/2019/06/30/spark-sql-programming/</id>
    <published>2019-06-30T10:18:16.000Z</published>
    <updated>2019-07-05T10:13:47.486Z</updated>
    
    <content type="html"><![CDATA[<div class="toc"><!-- toc --><ul><li><a href="#spark-xue-xi-san-spark-sql-xiang-jie">Spark学习（三）Spark-SQL详解</a><ul><li><a href="#spark-sql-jian-jie">Spark SQL 简介</a><ul><li><a href="#spark-sql-jia-gou">Spark SQL 架构</a></li></ul></li><li><a href="#spark-sql-ji-ben-shi-yong">Spark SQL 基本使用</a><ul><li><a href="#sparksession-zhu-ru-kou">SparkSession主入口</a></li><li><a href="#chuang-jian-dataframes">创建DataFrames</a></li><li><a href="#dataframe-ji-ben-cao-zuo">DataFrame 基本操作</a></li><li><a href="#zhi-xing-dong-tai-bian-cheng-cha-xun-sql">执行动态编程查询SQL</a></li><li><a href="#quan-ju-lin-shi-shi-tu">全局临时视图</a></li><li><a href="#chuang-jian-shu-ju-ji">创建数据集</a></li><li><a href="#jiang-rdd-zhuan-huan-cheng-dataset">将RDD转换成Dataset</a><ul><li><a href="#shi-yong-fan-she-tui-duan-mo-shi">使用反射推断模式</a></li><li><a href="#yi-bian-cheng-fang-shi-zhi-ding-mo-shi">以编程方式指定模式</a></li></ul></li></ul></li><li><a href="#datasource-shu-ju-yuan">DataSource数据源</a></li></ul></li></ul><!-- tocstop --></div><h1><span id="spark-xue-xi-san-spark-sql-xiang-jie">Spark学习（三）Spark-SQL详解</span><a href="#spark-xue-xi-san-spark-sql-xiang-jie" class="header-anchor"></a></h1><p>@(BigData)[Spark, 大数据]</p><h2><span id="spark-sql-jian-jie">Spark SQL 简介</span><a href="#spark-sql-jian-jie" class="header-anchor"></a></h2><p>Spark SQL是一个用于结构化数据处理的Spark模块。与基本的Spark RDD API不同，Spark SQL提供的接口为Spark提供了关于数据结构和正在执行的计算的更多信息。</p><p>本文所有源代码可以访问<a href="https://github.com/ChangCheng-Lei/Java-Spark-Tutorial" target="_blank" rel="noopener">GIT HUB</a>获取</p><ol><li><strong>Datasets</strong>是分布式的数据集合，Dataset 支持Scala 和 JAVA的访问</li><li><strong>DataFrame</strong> 是组织成命名列的数据集</li></ol><h3><span id="spark-sql-jia-gou">Spark SQL 架构</span><a href="#spark-sql-jia-gou" class="header-anchor"></a></h3><p><img src="https://i.loli.net/2019/07/05/5d1f20e27348d22212.png" alt="1561425410767.png"></p><h2><span id="spark-sql-ji-ben-shi-yong">Spark SQL 基本使用</span><a href="#spark-sql-ji-ben-shi-yong" class="header-anchor"></a></h2><h3><span id="sparksession-zhu-ru-kou">SparkSession主入口</span><a href="#sparksession-zhu-ru-kou" class="header-anchor"></a></h3><p>Spark SQL 程序的主入口是SparkSession, 所以其余操作是基于SparkSession之上的，创建SparkSession代码如下</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">SparkSession sparkSession = SparkSession.builder()</span><br><span class="line">                .appName(<span class="string">"Java Spark SQL basic example"</span>)</span><br><span class="line">                .master(<span class="string">"local"</span>)</span><br><span class="line">                .config(<span class="string">"spark.some.config.option"</span>, <span class="string">"some-value"</span>)</span><br><span class="line">                .getOrCreate();</span><br></pre></td></tr></table></figure><h3><span id="chuang-jian-dataframes">创建DataFrames</span><a href="#chuang-jian-dataframes" class="header-anchor"></a></h3><p>通过<em>SparkSession</em>, 程序可以通过已经存在的RDD, Hive table, Spark data sources 来创建DataFrame。<br>一下程序就是通过Json文件创建Dataset</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.sql.Dataset;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.Row;</span><br><span class="line"></span><br><span class="line">Dataset&lt;Row&gt; df = spark.read().json(<span class="string">"examples/src/main/resources/people.json"</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">// Displays the content of the DataFrame to stdout</span></span><br><span class="line">df.show();</span><br><span class="line"><span class="comment">// +----+-------+</span></span><br><span class="line"><span class="comment">// | age|   name|</span></span><br><span class="line"><span class="comment">// +----+-------+</span></span><br><span class="line"><span class="comment">// |null|Michael|</span></span><br><span class="line"><span class="comment">// |  30|   Andy|</span></span><br><span class="line"><span class="comment">// |  19| Justin|</span></span><br><span class="line"><span class="comment">// +----+-</span></span><br></pre></td></tr></table></figure><h3><span id="dataframe-ji-ben-cao-zuo">DataFrame 基本操作</span><a href="#dataframe-ji-ben-cao-zuo" class="header-anchor"></a></h3><p>DataFrame 基本操作类似于关系型数据库SQL操作,所有的操作可以查看<a href="http://spark.apache.org/docs/latest/api/java/org/apache/spark/sql/Dataset.html" target="_blank" rel="noopener">DataSet JAVA API</a></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// col("...") is preferable to df.col("...")</span></span><br><span class="line"><span class="keyword">import</span> <span class="keyword">static</span> org.apache.spark.sql.functions.col;</span><br><span class="line"></span><br><span class="line"><span class="comment">// Print the schema in a tree format</span></span><br><span class="line">df.printSchema();</span><br><span class="line"><span class="comment">// root</span></span><br><span class="line"><span class="comment">// |-- age: long (nullable = true)</span></span><br><span class="line"><span class="comment">// |-- name: string (nullable = true)</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// Select only the "name" column</span></span><br><span class="line">df.select(<span class="string">"name"</span>).show();</span><br><span class="line"><span class="comment">// +-------+</span></span><br><span class="line"><span class="comment">// |   name|</span></span><br><span class="line"><span class="comment">// +-------+</span></span><br><span class="line"><span class="comment">// |Michael|</span></span><br><span class="line"><span class="comment">// |   Andy|</span></span><br><span class="line"><span class="comment">// | Justin|</span></span><br><span class="line"><span class="comment">// +-------+</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// Select everybody, but increment the age by 1</span></span><br><span class="line">df.select(col(<span class="string">"name"</span>), col(<span class="string">"age"</span>).plus(<span class="number">1</span>)).show();</span><br><span class="line"><span class="comment">// +-------+---------+</span></span><br><span class="line"><span class="comment">// |   name|(age + 1)|</span></span><br><span class="line"><span class="comment">// +-------+---------+</span></span><br><span class="line"><span class="comment">// |Michael|     null|</span></span><br><span class="line"><span class="comment">// |   Andy|       31|</span></span><br><span class="line"><span class="comment">// | Justin|       20|</span></span><br><span class="line"><span class="comment">// +-------+---------+</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// Select people older than 21</span></span><br><span class="line">df.filter(col(<span class="string">"age"</span>).gt(<span class="number">21</span>)).show();</span><br><span class="line"><span class="comment">// +---+----+</span></span><br><span class="line"><span class="comment">// |age|name|</span></span><br><span class="line"><span class="comment">// +---+----+</span></span><br><span class="line"><span class="comment">// | 30|Andy|</span></span><br><span class="line"><span class="comment">// +---+----+</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// Count people by age</span></span><br><span class="line">df.groupBy(<span class="string">"age"</span>).count().show();</span><br><span class="line"><span class="comment">// +----+-----+</span></span><br><span class="line"><span class="comment">// | age|count|</span></span><br><span class="line"><span class="comment">// +----+-----+</span></span><br><span class="line"><span class="comment">// |  19|    1|</span></span><br><span class="line"><span class="comment">// |null|    1|</span></span><br><span class="line"><span class="comment">// |  30|    1|</span></span><br><span class="line"><span class="comment">// +----+-----+</span></span><br></pre></td></tr></table></figure><h3><span id="zhi-xing-dong-tai-bian-cheng-cha-xun-sql">执行动态编程查询SQL</span><a href="#zhi-xing-dong-tai-bian-cheng-cha-xun-sql" class="header-anchor"></a></h3><p><em>sql</em>方法基于SparkSession之上，可以定置化查询语句，其返回结果是<strong>Dataset<row></row></strong></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.sql.Dataset;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.Row;</span><br><span class="line"></span><br><span class="line"><span class="comment">// Register the DataFrame as a SQL temporary view</span></span><br><span class="line">df.createOrReplaceTempView(<span class="string">"people"</span>);</span><br><span class="line"></span><br><span class="line">Dataset&lt;Row&gt; sqlDF = spark.sql(<span class="string">"SELECT * FROM people"</span>);</span><br><span class="line">sqlDF.show();</span><br><span class="line"><span class="comment">// +----+-------+</span></span><br><span class="line"><span class="comment">// | age|   name|</span></span><br><span class="line"><span class="comment">// +----+-------+</span></span><br><span class="line"><span class="comment">// |null|Michael|</span></span><br><span class="line"><span class="comment">// |  30|   Andy|</span></span><br><span class="line"><span class="comment">// |  19| Justin|</span></span><br><span class="line"><span class="comment">// +----+-------+</span></span><br></pre></td></tr></table></figure><h3><span id="quan-ju-lin-shi-shi-tu">全局临时视图</span><a href="#quan-ju-lin-shi-shi-tu" class="header-anchor"></a></h3><p>Spark 提供视图功能，分为<strong>临时视图</strong>和<strong>全局临时视图</strong>，<em>临时视图</em>是随着创建Session的终止而消亡，如果需要创建一个临时视图在所有session当中共享，直到Spark服务终止才会消亡，则可以创建一个<em>全局临时视图</em>，<em>全局临时视图</em>会将数据保存到<em>global_temp</em>数据库中，使用时候必须显示的指定数据库，例如<br>SELECT  * FROM global_temp.view1<br>示例代码：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Register the DataFrame as a global temporary view</span></span><br><span class="line">df.createGlobalTempView(<span class="string">"people"</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">// Global temporary view is tied to a system preserved database `global_temp`</span></span><br><span class="line">spark.sql(<span class="string">"SELECT * FROM global_temp.people"</span>).show();</span><br><span class="line"><span class="comment">// +----+-------+</span></span><br><span class="line"><span class="comment">// | age|   name|</span></span><br><span class="line"><span class="comment">// +----+-------+</span></span><br><span class="line"><span class="comment">// |null|Michael|</span></span><br><span class="line"><span class="comment">// |  30|   Andy|</span></span><br><span class="line"><span class="comment">// |  19| Justin|</span></span><br><span class="line"><span class="comment">// +----+-------+</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// Global temporary view is cross-session</span></span><br><span class="line">spark.newSession().sql(<span class="string">"SELECT * FROM global_temp.people"</span>).show();</span><br><span class="line"><span class="comment">// +----+-------+</span></span><br><span class="line"><span class="comment">// | age|   name|</span></span><br><span class="line"><span class="comment">// +----+-------+</span></span><br><span class="line"><span class="comment">// |null|Michael|</span></span><br><span class="line"><span class="comment">// |  30|   Andy|</span></span><br><span class="line"><span class="comment">// |  19| Justin|</span></span><br><span class="line"><span class="comment">// +----+-------+</span></span><br></pre></td></tr></table></figure><h3><span id="chuang-jian-shu-ju-ji">创建数据集</span><a href="#chuang-jian-shu-ju-ji" class="header-anchor"></a></h3><p>数据集与RDDs类似，它们不是使用Java序列化或Kryo，而是使用专门的编码器序列化对象，以便通过网络进行处理或传输。虽然编码器和标准序列化都负责将对象转换为字节，但编码器是动态生成的代码，使用的格式允许Spark执行许多操作，比如过滤、排序和散列，而无需将字节反序列化回对象。</p><p>示例代码</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> java.util.Arrays;</span><br><span class="line"><span class="keyword">import</span> java.util.Collections;</span><br><span class="line"><span class="keyword">import</span> java.io.Serializable;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.function.MapFunction;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.Dataset;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.Row;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.Encoder;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.Encoders;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">Person</span> <span class="keyword">implements</span> <span class="title">Serializable</span> </span>&#123;</span><br><span class="line">  <span class="keyword">private</span> String name;</span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">int</span> age;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">public</span> String <span class="title">getName</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> name;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setName</span><span class="params">(String name)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">this</span>.name = name;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">getAge</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> age;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setAge</span><span class="params">(<span class="keyword">int</span> age)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">this</span>.age = age;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// Create an instance of a Bean class</span></span><br><span class="line">Person person = <span class="keyword">new</span> Person();</span><br><span class="line">person.setName(<span class="string">"Andy"</span>);</span><br><span class="line">person.setAge(<span class="number">32</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">// Encoders are created for Java beans</span></span><br><span class="line">Encoder&lt;Person&gt; personEncoder = Encoders.bean(Person.class);</span><br><span class="line">Dataset&lt;Person&gt; javaBeanDS = spark.createDataset(</span><br><span class="line">  Collections.singletonList(person),</span><br><span class="line">  personEncoder</span><br><span class="line">);</span><br><span class="line">javaBeanDS.show();</span><br><span class="line"><span class="comment">// +---+----+</span></span><br><span class="line"><span class="comment">// |age|name|</span></span><br><span class="line"><span class="comment">// +---+----+</span></span><br><span class="line"><span class="comment">// | 32|Andy|</span></span><br><span class="line"><span class="comment">// +---+----+</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// Encoders for most common types are provided in class Encoders</span></span><br><span class="line">Encoder&lt;Integer&gt; integerEncoder = Encoders.INT();</span><br><span class="line">Dataset&lt;Integer&gt; primitiveDS = spark.createDataset(Arrays.asList(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>), integerEncoder);</span><br><span class="line">Dataset&lt;Integer&gt; transformedDS = primitiveDS.map(</span><br><span class="line">    (MapFunction&lt;Integer, Integer&gt;) value -&gt; value + <span class="number">1</span>,</span><br><span class="line">    integerEncoder);</span><br><span class="line">transformedDS.collect(); <span class="comment">// Returns [2, 3, 4]</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// DataFrames can be converted to a Dataset by providing a class. Mapping based on name</span></span><br><span class="line">String path = <span class="string">"examples/src/main/resources/people.json"</span>;</span><br><span class="line">Dataset&lt;Person&gt; peopleDS = spark.read().json(path).as(personEncoder);</span><br><span class="line">peopleDS.show();</span><br><span class="line"><span class="comment">// +----+-------+</span></span><br><span class="line"><span class="comment">// | age|   name|</span></span><br><span class="line"><span class="comment">// +----+-------+</span></span><br><span class="line"><span class="comment">// |null|Michael|</span></span><br><span class="line"><span class="comment">// |  30|   Andy|</span></span><br><span class="line"><span class="comment">// |  19| Justin|</span></span><br><span class="line"><span class="comment">// +----+-------+</span></span><br></pre></td></tr></table></figure><h3><span id="jiang-rdd-zhuan-huan-cheng-dataset">将RDD转换成Dataset</span><a href="#jiang-rdd-zhuan-huan-cheng-dataset" class="header-anchor"></a></h3><p>Spark 支持两种方式将RDD转换成Dataset.</p><ol><li>第一种方法使用反射来推断包含特定对象类型的RDD的模式。这种基于反射的方法可以生成更简洁的代码，并且当您在编写Spark应用程序时已经知道模式时，这种方法可以很好地工作。</li><li>创建数据集的第二种方法是通过编程接口，该接口允许您构造模式，然后将其应用于现有的RDD。虽然这个方法更详细，但它允许您在列及其类型直到运行时才知道时构造数据集。</li></ol><h4><span id="shi-yong-fan-she-tui-duan-mo-shi">使用反射推断模式</span><a href="#shi-yong-fan-she-tui-duan-mo-shi" class="header-anchor"></a></h4><p>示例代码：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.JavaRDD;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.function.Function;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.function.MapFunction;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.Dataset;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.Row;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.Encoder;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.Encoders;</span><br><span class="line"></span><br><span class="line"><span class="comment">// Create an RDD of Person objects from a text file</span></span><br><span class="line">JavaRDD&lt;Person&gt; peopleRDD = spark.read()</span><br><span class="line">  .textFile(<span class="string">"examples/src/main/resources/people.txt"</span>)</span><br><span class="line">  .javaRDD()</span><br><span class="line">  .map(line -&gt; &#123;</span><br><span class="line">    String[] parts = line.split(<span class="string">","</span>);</span><br><span class="line">    Person person = <span class="keyword">new</span> Person();</span><br><span class="line">    person.setName(parts[<span class="number">0</span>]);</span><br><span class="line">    person.setAge(Integer.parseInt(parts[<span class="number">1</span>].trim()));</span><br><span class="line">    <span class="keyword">return</span> person;</span><br><span class="line">  &#125;);</span><br><span class="line"></span><br><span class="line"><span class="comment">// Apply a schema to an RDD of JavaBeans to get a DataFrame</span></span><br><span class="line">Dataset&lt;Row&gt; peopleDF = spark.createDataFrame(peopleRDD, Person.class);</span><br><span class="line"><span class="comment">// Register the DataFrame as a temporary view</span></span><br><span class="line">peopleDF.createOrReplaceTempView(<span class="string">"people"</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">// SQL statements can be run by using the sql methods provided by spark</span></span><br><span class="line">Dataset&lt;Row&gt; teenagersDF = spark.sql(<span class="string">"SELECT name FROM people WHERE age BETWEEN 13 AND 19"</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">// The columns of a row in the result can be accessed by field index</span></span><br><span class="line">Encoder&lt;String&gt; stringEncoder = Encoders.STRING();</span><br><span class="line">Dataset&lt;String&gt; teenagerNamesByIndexDF = teenagersDF.map(</span><br><span class="line">    (MapFunction&lt;Row, String&gt;) row -&gt; <span class="string">"Name: "</span> + row.getString(<span class="number">0</span>),</span><br><span class="line">    stringEncoder);</span><br><span class="line">teenagerNamesByIndexDF.show();</span><br><span class="line"><span class="comment">// +------------+</span></span><br><span class="line"><span class="comment">// |       value|</span></span><br><span class="line"><span class="comment">// +------------+</span></span><br><span class="line"><span class="comment">// |Name: Justin|</span></span><br><span class="line"><span class="comment">// +------------+</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// or by field name</span></span><br><span class="line">Dataset&lt;String&gt; teenagerNamesByFieldDF = teenagersDF.map(</span><br><span class="line">    (MapFunction&lt;Row, String&gt;) row -&gt; <span class="string">"Name: "</span> + row.&lt;String&gt;getAs(<span class="string">"name"</span>),</span><br><span class="line">    stringEncoder);</span><br><span class="line">teenagerNamesByFieldDF.show();</span><br><span class="line"><span class="comment">// +------------+</span></span><br><span class="line"><span class="comment">// |       value|</span></span><br><span class="line"><span class="comment">// +------------+</span></span><br><span class="line"><span class="comment">// |Name: Justin|</span></span><br><span class="line"><span class="comment">// +------------+</span></span><br></pre></td></tr></table></figure><h4><span id="yi-bian-cheng-fang-shi-zhi-ding-mo-shi">以编程方式指定模式</span><a href="#yi-bian-cheng-fang-shi-zhi-ding-mo-shi" class="header-anchor"></a></h4><p>示例代码</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line">mport java.util.ArrayList;</span><br><span class="line"><span class="keyword">import</span> java.util.List;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.JavaRDD;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.function.Function;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.Dataset;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.Row;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.types.DataTypes;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.types.StructField;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.types.StructType;</span><br><span class="line"></span><br><span class="line"><span class="comment">// Create an RDD</span></span><br><span class="line">JavaRDD&lt;String&gt; peopleRDD = spark.sparkContext()</span><br><span class="line">  .textFile(<span class="string">"examples/src/main/resources/people.txt"</span>, <span class="number">1</span>)</span><br><span class="line">  .toJavaRDD();</span><br><span class="line"></span><br><span class="line"><span class="comment">// The schema is encoded in a string</span></span><br><span class="line">String schemaString = <span class="string">"name age"</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">// Generate the schema based on the string of schema</span></span><br><span class="line">List&lt;StructField&gt; fields = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line"><span class="keyword">for</span> (String fieldName : schemaString.split(<span class="string">" "</span>)) &#123;</span><br><span class="line">StructField field = DataTypes.createStructField(fieldName, DataTypes.StringType, <span class="keyword">true</span>);</span><br><span class="line">  fields.add(field);</span><br><span class="line">&#125;</span><br><span class="line">StructType schema = DataTypes.createStructType(fields);</span><br><span class="line"></span><br><span class="line"><span class="comment">// Convert records of the RDD (people) to Rows</span></span><br><span class="line">JavaRDD&lt;Row&gt; rowRDD = peopleRDD.map((Function&lt;String, Row&gt;) record -&gt; &#123;</span><br><span class="line">  String[] attributes = record.split(<span class="string">","</span>);</span><br><span class="line">  <span class="keyword">return</span> RowFactory.create(attributes[<span class="number">0</span>], attributes[<span class="number">1</span>].trim());</span><br><span class="line">&#125;);</span><br><span class="line"></span><br><span class="line"><span class="comment">// Apply the schema to the RDD</span></span><br><span class="line">Dataset&lt;Row&gt; peopleDataFrame = spark.createDataFrame(rowRDD, schema);</span><br><span class="line"></span><br><span class="line"><span class="comment">// Creates a temporary view using the DataFrame</span></span><br><span class="line">peopleDataFrame.createOrReplaceTempView(<span class="string">"people"</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">// SQL can be run over a temporary view created using DataFrames</span></span><br><span class="line">Dataset&lt;Row&gt; results = spark.sql(<span class="string">"SELECT name FROM people"</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">// The results of SQL queries are DataFrames and support all the normal RDD operations</span></span><br><span class="line"><span class="comment">// The columns of a row in the result can be accessed by field index or by field name</span></span><br><span class="line">Dataset&lt;String&gt; namesDS = results.map(</span><br><span class="line">    (MapFunction&lt;Row, String&gt;) row -&gt; <span class="string">"Name: "</span> + row.getString(<span class="number">0</span>),</span><br><span class="line">    Encoders.STRING());</span><br><span class="line">namesDS.show();</span><br><span class="line"><span class="comment">// +-------------+</span></span><br><span class="line"><span class="comment">// |        value|</span></span><br><span class="line"><span class="comment">// +-------------+</span></span><br><span class="line"><span class="comment">// |Name: Michael|</span></span><br><span class="line"><span class="comment">// |   Name: Andy|</span></span><br><span class="line"><span class="comment">// | Name: Justin|</span></span><br><span class="line"><span class="comment">// +-------------+</span></span><br></pre></td></tr></table></figure><h2><span id="datasource-shu-ju-yuan">DataSource数据源</span><a href="#datasource-shu-ju-yuan" class="header-anchor"></a></h2><p>Spark SQL支持通过DataFrame interfac对各种数据源进行操作，现在支持<strong>ORC文件</strong>，<strong>JSON文件</strong>，<strong>HIVE table</strong>,<strong>JDBC</strong>,<strong>Avro文件</strong><br>详细文档参考<a href="http://spark.apache.org/docs/latest/sql-data-sources.html" target="_blank" rel="noopener">官方文档</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;div class=&quot;toc&quot;&gt;

&lt;!-- toc --&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;#spark-xue-xi-san-spark-sql-xiang-jie&quot;&gt;Spark学习（三）Spark-SQL详解&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;#spark-
      
    
    </summary>
    
    
      <category term="Spark" scheme="http://yoursite.com/tags/Spark/"/>
    
      <category term="大数据" scheme="http://yoursite.com/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
  </entry>
  
  <entry>
    <title>Spark学习（二）RDD详解</title>
    <link href="http://yoursite.com/2019/06/23/spark-rdd-programming/"/>
    <id>http://yoursite.com/2019/06/23/spark-rdd-programming/</id>
    <published>2019-06-23T08:18:16.000Z</published>
    <updated>2019-07-05T06:24:26.200Z</updated>
    
    <content type="html"><![CDATA[<div class="toc"><!-- toc --><ul><li><a href="#rdd-jian-jie">RDD 简介</a><ul><li><a href="#ji-ben-gai-nian">基本概念</a></li><li><a href="#chuang-jian-rdd">创建RDD</a><ul><li><a href="#parallelized-collections">Parallelized Collections</a></li><li><a href="#external-datasets-cong-wai-bu-wen-jian-du-qu">External Datasets 从外部文件读取</a></li></ul></li></ul></li><li><a href="#rdd-cao-zuo">RDD 操作</a><ul><li><a href="#transformation">Transformation</a><ul><li><a href="#flatmap-he-map">flatMap 和 Map</a></li><li><a href="#sample-cao-zuo">sample 操作</a></li><li><a href="#distinct-cao-zuo">distinct 操作</a></li><li><a href="#union-he-bing-cao-zuo">union 合并操作</a></li><li><a href="#intersection-jiao-ji">intersection 交集</a></li><li><a href="#substract-zi-ji">substract 子集</a></li><li><a href="#cartesian-di-qia-er-ji">cartesian 笛卡尔积</a></li></ul></li><li><a href="#actions">Actions</a><ul><li><a href="#collect-cao-zuo">collect 操作</a></li><li><a href="#count-he-countbyvalue">count 和 countByValue</a></li><li><a href="#take-cao-zuo">take 操作</a></li><li><a href="#saveastextfile-cao-zuo">saveAsTextFile 操作</a></li><li><a href="#reduce-cao-zuo">reduce 操作</a></li></ul></li></ul></li></ul><!-- tocstop --></div><h2><span id="rdd-jian-jie">RDD 简介</span><a href="#rdd-jian-jie" class="header-anchor"></a></h2><blockquote><p>Resilient Distributed Dataset 意思是弹性分布式数据集,是Spark 中最基本的数据抽象</p></blockquote><p>本文所有源代码可以访问<a href="https://github.com/ChangCheng-Lei/Java-Spark-Tutorial" target="_blank" rel="noopener">GIT HUB</a> 获取</p><h3><span id="ji-ben-gai-nian">基本概念</span><a href="#ji-ben-gai-nian" class="header-anchor"></a></h3><ul><li>dataset : dataset 是一系列数据的集合，它的展现形式可以是Strings,  integers 甚至是存放在关系型数据库中的数据 </li></ul><h3><span id="chuang-jian-rdd">创建RDD</span><a href="#chuang-jian-rdd" class="header-anchor"></a></h3><blockquote><p>创建RDD 有两种方式，<em>Parallelized Collections</em> 和 <em>External Datasets</em></p></blockquote><p>在JAVA maven 项目中， 添加依赖即可引用spark 相关类</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.spark<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spark-core_2.12<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.4.3<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure><p>在maven 项目一般使用方式如下</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.lcc.spark;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.SparkConf;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.JavaSparkContext;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">HelloWordSpark</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        String appName = "HellowWorld"; #项目名称</span><br><span class="line">        String master = "local"; # 运行模式</span><br><span class="line">        SparkConf conf = <span class="keyword">new</span> SparkConf().setAppName(appName).setMaster(master);</span><br><span class="line">        JavaSparkContext sc = <span class="keyword">new</span> JavaSparkContext(conf);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4><span id="parallelized-collections">Parallelized Collections</span><a href="#parallelized-collections" class="header-anchor"></a></h4><p>将现有的数据直接转换成RDD , 但是一般使用场景不太会用到</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">List&lt;Integer&gt; data = Arrays.asList(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>);</span><br><span class="line">JavaRDD&lt;Integer&gt; distData = sc.parallelize(data);</span><br></pre></td></tr></table></figure><h4><span id="external-datasets-cong-wai-bu-wen-jian-du-qu">External Datasets 从外部文件读取</span><a href="#external-datasets-cong-wai-bu-wen-jian-du-qu" class="header-anchor"></a></h4><p>一般情况下需要分析的Spark文件都比较大，存放在HDFS或者HBASe 等外部文件，比较适用实际使用场景<br>使用方法</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">JavaRDD&lt;String&gt; distFile = sc.textFile(<span class="string">"data.txt"</span>);</span><br></pre></td></tr></table></figure><h2><span id="rdd-cao-zuo">RDD 操作</span><a href="#rdd-cao-zuo" class="header-anchor"></a></h2><blockquote><p>RDD 主要包含transformaitons 和 actions 两个阶段的操作</p></blockquote><h3><span id="transformation">Transformation</span><a href="#transformation" class="header-anchor"></a></h3><blockquote><p>Transformation 主要目的是转换数据， 从已经存在的数据集上新创建一个</p></blockquote><p>transformations 是RDD上一系列操作，可以返回一个<strong>新的RDD</strong>，Spark 中所有的transformation都是采用的LAZY模式，他不会立马计算出结果，直到有操作需要计算新的RDD时才会进行计算，默认情况下，每一个transformed RDD 每次调用可能出现重复计算，可以通过<em>persist</em> RDD 到内存。</p><blockquote><p>注意 ： transformations 会返回一个新的Rdd</p></blockquote><p>最常见的transformation 操作时<strong>过滤filter</strong> 和 <strong>映射map</strong>操作<br>代码案例–统计文本中每个单词出现的次数</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.lcc.spark;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.SparkConf;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.JavaRDD;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.JavaSparkContext;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.function.FlatMapFunction;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.Arrays;</span><br><span class="line"><span class="keyword">import</span> java.util.Iterator;</span><br><span class="line"><span class="keyword">import</span> java.util.Map;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Hello world!</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">WordsCount</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        String appName = <span class="string">"HellowWorld"</span>;</span><br><span class="line">        String master = <span class="string">"local"</span>;</span><br><span class="line">        SparkConf conf = <span class="keyword">new</span> SparkConf().setAppName(appName).setMaster(master);</span><br><span class="line">        JavaSparkContext sparkContext = <span class="keyword">new</span> JavaSparkContext(conf);</span><br><span class="line">        JavaRDD&lt;String&gt; stringJavaRDD = sparkContext.textFile(<span class="string">"src/main/resources/input/word_count.text"</span>);</span><br><span class="line">        JavaRDD&lt;String&gt; objectJavaRDD = stringJavaRDD.flatMap(<span class="keyword">new</span> FlatMapFunction&lt;String, String&gt;() &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> Iterator&lt;String&gt; <span class="title">call</span><span class="params">(String s)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                <span class="keyword">return</span> Arrays.asList(s.split(<span class="string">" "</span>)).iterator();</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line">        Map&lt;String, Long&gt; stringLongMap = objectJavaRDD.countByValue();</span><br><span class="line">        <span class="keyword">for</span> (Map.Entry&lt;String, Long&gt; entry : stringLongMap.entrySet()) &#123;</span><br><span class="line">            System.out.println(<span class="string">"Key = "</span> + entry.getKey() + <span class="string">", Value = "</span> + entry.getValue());</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4><span id="flatmap-he-map">flatMap 和 Map</span><a href="#flatmap-he-map" class="header-anchor"></a></h4><p><img src="https://i.loli.net/2019/07/05/5d1eeb31393e167850.png" alt="1560780323084.png"><br><strong>Map</strong>适用场景为<strong>1对1</strong>映射关系<br><img src="https://i.loli.net/2019/07/05/5d1eeb307fa8d34013.png" alt="1560780356961.png"><br><strong>FlatMap</strong>适用场景是<strong>1对多</strong>映射关系<br><img src="https://i.loli.net/2019/07/05/5d1eeb30ccd7c49936.png" alt="1560780392131.png"></p><h4><span id="sample-cao-zuo">sample 操作</span><a href="#sample-cao-zuo" class="header-anchor"></a></h4><p>sample 操作会返回一个随机的rdd， 常用语获取随机数据</p><h4><span id="distinct-cao-zuo">distinct 操作</span><a href="#distinct-cao-zuo" class="header-anchor"></a></h4><p>返回唯一的distinct操作</p><h4><span id="union-he-bing-cao-zuo">union 合并操作</span><a href="#union-he-bing-cao-zuo" class="header-anchor"></a></h4><p>union 操作实质是求并集合， union = A U B</p><h4><span id="intersection-jiao-ji">intersection 交集</span><a href="#intersection-jiao-ji" class="header-anchor"></a></h4><p>intersection operation 操作实质是求交集 intersection = A∩B</p><blockquote><p>注意 intersection 操作时非常消耗资源的，因为需要把所有的分区都遍历才能获取到交集<br>所获取到的交集结果会去重处理</p></blockquote><h4><span id="substract-zi-ji">substract 子集</span><a href="#substract-zi-ji" class="header-anchor"></a></h4><blockquote><p>注意 intersection 操作时非常消耗资源的，因为需要把所有的分区都遍历才能获取到子集</p></blockquote><h4><span id="cartesian-di-qia-er-ji">cartesian 笛卡尔积</span><a href="#cartesian-di-qia-er-ji" class="header-anchor"></a></h4><p>返回RDD A 和 RDD B 笛卡尔积</p><h3><span id="actions">Actions</span><a href="#actions" class="header-anchor"></a></h3><blockquote><p>actions是将最终值返回给驱动程序或将数据持久化到外部存储系统的操作</p></blockquote><h4><span id="collect-cao-zuo">collect 操作</span><a href="#collect-cao-zuo" class="header-anchor"></a></h4><h4><span id="count-he-countbyvalue">count 和 countByValue</span><a href="#count-he-countbyvalue" class="header-anchor"></a></h4><ul><li><strong>count</strong> 用于统计有多行数据在当前rdd当中，count 会返回有多少元素</li><li><strong>countByValue</strong> 会在当前RDD下，按照唯一值进行计数， 并返回一个map(这个很类似mysql的count函数)</li></ul><h4><span id="take-cao-zuo">take 操作</span><a href="#take-cao-zuo" class="header-anchor"></a></h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">words = wordRdd.tabke(<span class="number">3</span>)</span><br></pre></td></tr></table></figure><ul><li>take action 从当前RDD 当中获取N个元素</li><li>take action 常用语单元测试和debug</li></ul><h4><span id="saveastextfile-cao-zuo">saveAsTextFile 操作</span><a href="#saveastextfile-cao-zuo" class="header-anchor"></a></h4><p>saveAsTextFile操作主要是将数据存放到硬盘中，这个查询之前的即可</p><h4><span id="reduce-cao-zuo">reduce 操作</span><a href="#reduce-cao-zuo" class="header-anchor"></a></h4><p>reduce 操作与mapReduce 中思想一样</p><p><img src="https://i.loli.net/2019/07/05/5d1eeb311fd2313743.png" alt="1560780429240.png"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;div class=&quot;toc&quot;&gt;

&lt;!-- toc --&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;#rdd-jian-jie&quot;&gt;RDD 简介&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;#ji-ben-gai-nian&quot;&gt;基本概念&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#
      
    
    </summary>
    
    
      <category term="Spark" scheme="http://yoursite.com/tags/Spark/"/>
    
      <category term="大数据" scheme="http://yoursite.com/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
  </entry>
  
  <entry>
    <title>Spark学习（1）安装Spark</title>
    <link href="http://yoursite.com/2019/06/16/sparkInstall/"/>
    <id>http://yoursite.com/2019/06/16/sparkInstall/</id>
    <published>2019-06-16T08:18:16.000Z</published>
    <updated>2019-06-16T08:56:41.341Z</updated>
    
    <content type="html"><![CDATA[<div class="toc"><!-- toc --><ul><li><a href="#spark-jian-jie">Spark 简介</a></li><li><a href="#spark-an-zhuang">Spark 安装</a><ul><li><a href="#cai-yong-an-zhuang-bao-de-fang-shi-an-zhuang">采用安装包的方式安装</a><ul><li><a href="#linux-shang-an-zhuang-spark">LINUX 上安装Spark</a></li><li><a href="#windows-an-zhuang-spark">Windows 安装Spark</a></li><li><a href="#cai-yong-yuan-ma-fang-shi-an-zhuang">采用源码方式安装</a></li></ul></li></ul></li></ul><!-- tocstop --></div><h2><span id="spark-jian-jie">Spark 简介</span><a href="#spark-jian-jie" class="header-anchor"></a></h2><blockquote><p>Apache Spark™ is a unified analytics engine for large-scale data processing.<br>若需要查询更多信息，访问Spark <a href="https://spark.apache.org/" target="_blank" rel="noopener">官方网站</a><br>若需要查询Spark源码，访问Spark <a href="https://github.com/apache/spark" target="_blank" rel="noopener">Git Hub</a></p></blockquote><p>Spark 是一个快速且通用的集群计算平台</p><ul><li>Spark 是快速的 Spark 扩充了流行MapReduce 计算模型， 基于内存计算</li><li>Spark 通用的， 容纳了其它分布式系统拥有的功能，批处理，迭代式计算，交互式查询和流处理，降低集群维护成本</li><li>高度开放的，提供了Python, Java等高级API</li></ul><h2><span id="spark-an-zhuang">Spark 安装</span><a href="#spark-an-zhuang" class="header-anchor"></a></h2><p>Spark 安装方式提供两种安装模式，<strong>编译源码安装</strong> 和 <strong>安装包方式安装</strong>示例安装系统为windows，Linux , 参照对应操作系统安装即可</p><p>系统环境要求</p><ol><li><strong>安装JAVA JDK1.8</strong><br>为什么需要安装JAVA JDK1.8 ？</li></ol><ul><li>Spark 是采用Scala 语言编写，Scala语言是运行在JVM上的</li><li>Python API（PySpark） 与Spark进行交互, PySpark 是在<em>Spark‘s JAVA API</em>之上的封装,JAVA API 也是需要JDK</li><li>JAVA JDK 版本要求是1.8</li></ul><h3><span id="cai-yong-an-zhuang-bao-de-fang-shi-an-zhuang">采用安装包的方式安装</span><a href="#cai-yong-an-zhuang-bao-de-fang-shi-an-zhuang" class="header-anchor"></a></h3><ol><li>打开Spark <a href="https://spark.apache.org/" target="_blank" rel="noopener">官方网站</a>，点击DownLoad, 这里已经Spark 已经集成成功了hadoop组件，可以直接使用<br><img src="https://i.loli.net/2019/06/16/5d05fd9d8fa2f41078.png" alt="1.png"></li></ol><h4><span id="linux-shang-an-zhuang-spark">LINUX 上安装Spark</span><a href="#linux-shang-an-zhuang-spark" class="header-anchor"></a></h4><figure class="highlight powershell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[root@Amy ~]<span class="comment"># mkdir download</span></span><br><span class="line">[root@Amy ~]<span class="comment"># cd download/</span></span><br><span class="line"><span class="comment">#wget &lt;spark_url&gt; //使用wget 命令下载spark</span></span><br><span class="line">[root@Amy download]<span class="comment">#</span></span><br><span class="line">wget http://mirror.bit.edu.cn/apache/spark/spark-<span class="number">2.4</span>.<span class="number">3</span>/spark-<span class="number">2.4</span>.<span class="number">3</span>-bin-hadoop2.<span class="number">7</span>.tgz</span><br><span class="line">mkdir /opt/apache-spark <span class="comment"># spakr 解压目录</span></span><br><span class="line">[root@Amy download]<span class="comment"># tar xvzf spark-2.4.3-bin-hadoop2.7.tgz -C /opt/apache-spark/</span></span><br></pre></td></tr></table></figure><ol start="3"><li>配置环境变量<figure class="highlight powershell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">[root@Amy download]<span class="comment"># cd /opt/apache-spark/spark-2.4.3-bin-hadoop2.7/</span></span><br><span class="line">[root@Amy spark-<span class="number">2.4</span>.<span class="number">3</span>-bin-hadoop2.<span class="number">7</span>]<span class="comment"># vim ~/.bashrc</span></span><br><span class="line"><span class="comment">#将Spark 环境配置加入</span></span><br><span class="line"><span class="comment">#export SPARK_HOME=/opt/apache-spark/spark-2.4.3-bin-hadoop2.7</span></span><br><span class="line"><span class="comment">#export PATH=$PATH:$SPARK_HOME/bin</span></span><br><span class="line"><span class="comment">#保存文件并退出</span></span><br><span class="line">[root@Amy spark-<span class="number">2.4</span>.<span class="number">3</span>-bin-hadoop2.<span class="number">7</span>]<span class="comment"># source ~/.bashrc # 使得环境变量生效</span></span><br><span class="line">[root@Amy spark-<span class="number">2.4</span>.<span class="number">3</span>-bin-hadoop2.<span class="number">7</span>]<span class="comment"># pyspark # 执行pySpark 命令</span></span><br></pre></td></tr></table></figure></li></ol><p>pySpark 命令正确执行结果如下<br><img src="https://i.loli.net/2019/06/16/5d05fd9d6762132822.png" alt="2.png"></p><h4><span id="windows-an-zhuang-spark">Windows 安装Spark</span><a href="#windows-an-zhuang-spark" class="header-anchor"></a></h4><ol><li><p>通过浏览下载刚刚选择的Spark 版本</p></li><li><p>将Spark解压到需要保存的文件目录<br>这里以D盘根目录为例<br><img src="https://i.loli.net/2019/06/16/5d05fd9db812640075.png" alt="3.png"></p><blockquote><p><strong>注意</strong> Spark 的安装与其它操作系统没有区别，但是由于Windows操作系统采用的是NTFS格式，hadoop 不能完全兼容所需，所以需要稍微进行处理</p></blockquote></li><li><p>下载<a href="https://github.com/jleetutorial/sparkTutorial/blob/winutils/winutils.exe" target="_blank" rel="noopener">工具</a><br>保存路径到<em>D:\hadoop\bin</em>（这里需要注意，后续配置环境变量需要）</p></li><li><p>配置环境变量</p></li></ol><ul><li>配置<em>HADOOP_HOME</em> 为 <em>D:\hadoop</em>（与上一步hadoop工具一致）</li><li>配置<em>SPARK_HOME *为</em>D:\spark-2.4.3-bin-hadoop2.7*（Spark 保存路径）</li><li>添加Hadoop 和Spark 到Path 中</li></ul><ol start="4"><li>校验安装是否成功</li></ol><ul><li>新创建一个文件目录<em>D:\tmp\hive</em>，使用工具修改文件夹格式<figure class="highlight crmsh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">C:\Users\Administrator&gt;winutils chmod <span class="number">777</span> D:\tmp\hive</span><br><span class="line">C:\Users\Administrator&gt;pyspark</span><br></pre></td></tr></table></figure></li></ul><p><img src="https://i.loli.net/2019/06/16/5d05fd9d7e00675647.png" alt="4.png"></p><h4><span id="cai-yong-yuan-ma-fang-shi-an-zhuang">采用源码方式安装</span><a href="#cai-yong-yuan-ma-fang-shi-an-zhuang" class="header-anchor"></a></h4><p>详细源码安装<a href="http://spark.apache.org/docs/latest/building-spark.html" target="_blank" rel="noopener">官方文档</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;div class=&quot;toc&quot;&gt;

&lt;!-- toc --&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;#spark-jian-jie&quot;&gt;Spark 简介&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#spark-an-zhuang&quot;&gt;Spark 安装&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a
      
    
    </summary>
    
    
      <category term="Spark" scheme="http://yoursite.com/tags/Spark/"/>
    
      <category term="大数据" scheme="http://yoursite.com/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
  </entry>
  
  <entry>
    <title>制作HEXO Docker 镜像</title>
    <link href="http://yoursite.com/2019/06/08/build-hexo-with-docker/"/>
    <id>http://yoursite.com/2019/06/08/build-hexo-with-docker/</id>
    <published>2019-06-08T03:15:56.000Z</published>
    <updated>2019-06-08T07:31:03.975Z</updated>
    
    <content type="html"><![CDATA[<div class="toc"><!-- toc --><ul><li><a href="#zhi-zuo-hexo-docker-jing-xiang">制作HEXO Docker镜像</a><ul><li><a href="#qi-dong-docker-rong-qi">启动docker 容器</a></li><li><a href="#pei-zhi-ubuntu-apt-get-shu-ju-yuan">配置ubuntu apt-get数据源</a></li><li><a href="#ubuntu-an-zhuang-git">ubuntu安装git</a></li><li><a href="#an-zhuang-node-js">安装node js</a><ul><li><a href="#huo-qu-node-js-ban-ben">获取node js 版本</a></li></ul></li><li><a href="#an-zhuang-hexo">安装hexo</a></li><li><a href="#jiang-rong-qi-da-cheng-jing-xiang">将容器打成镜像</a></li><li><a href="#pei-zhi-tu-pian-cha-jian">配置图片插件</a></li><li><a href="#can-kao-wen-dang">参考文档</a></li></ul></li></ul><!-- tocstop --></div><h2><span id="zhi-zuo-hexo-docker-jing-xiang">制作HEXO Docker镜像</span><a href="#zhi-zuo-hexo-docker-jing-xiang" class="header-anchor"></a></h2><p>Hexo是一个基于node.js的静态博客生成系统，它使用markdown语法来写作，同时支持丰富的自定义标签系统。但是对于后端开发者来讲， 本身对于Nodejs 不太熟悉，如果更换电脑又得捣腾一遍环境，所以笔者直接将hexo环境打成docker镜像，以便日后更好的使用</p><p>镜像路径 : <a href="https://hub.docker.com/r/changchenglei/hexo" target="_blank" rel="noopener">https://hub.docker.com/r/changchenglei/hexo</a><br>拉取镜像指令:  </p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker pull changchenglei/hexo</span><br></pre></td></tr></table></figure><p>本文采用ubuntu做为操作系统，进行容器化搭建.</p><h3><span id="qi-dong-docker-rong-qi">启动docker 容器</span><a href="#qi-dong-docker-rong-qi" class="header-anchor"></a></h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">docker pull ubuntu <span class="comment"># 拉取最新的ubuntu镜像</span></span><br><span class="line">docker run --name hexo_blog -d -i -t ubuntu <span class="comment">#启动容器</span></span><br><span class="line">docker <span class="built_in">exec</span> -it hexo_blog <span class="string">"/bin/bash"</span> <span class="comment">#进入容器进行安装</span></span><br></pre></td></tr></table></figure><h3><span id="pei-zhi-ubuntu-apt-get-shu-ju-yuan">配置ubuntu apt-get数据源</span><a href="#pei-zhi-ubuntu-apt-get-shu-ju-yuan" class="header-anchor"></a></h3><blockquote><p>当网络缓慢可以设置，若网络良好可以直接忽略<br>配置apt-get 数据源主要是为了加快下载速度，若网络速度已经很快可以不用修改</p></blockquote><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">docker run --name hexo_blog -d -i -t ubuntu</span><br><span class="line">sudo cp /etc/apt/sources.list /etc/apt/sources_init.list</span><br><span class="line">apt-get update</span><br><span class="line">apt-get upgrade</span><br><span class="line">apt-get install sudo</span><br><span class="line">apt-get install vim</span><br><span class="line">sudo vi /etc/apt/sources.list <span class="comment">#配置阿里数据源</span></span><br><span class="line">sudo apt-get update</span><br><span class="line">sudo apt-get -f install</span><br></pre></td></tr></table></figure><p>配置apt-get 阿里数据源， 不要删除原有的，将阿里数据源添加到最前面</p><figure class="highlight tex"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">deb http://mirrors.aliyun.com/ubuntu/ xenial main</span><br><span class="line">deb-src http://mirrors.aliyun.com/ubuntu/ xenial main</span><br><span class="line"></span><br><span class="line">deb http://mirrors.aliyun.com/ubuntu/ xenial-updates main</span><br><span class="line">deb-src http://mirrors.aliyun.com/ubuntu/ xenial-updates main</span><br><span class="line"></span><br><span class="line">deb http://mirrors.aliyun.com/ubuntu/ xenial universe</span><br><span class="line">deb-src http://mirrors.aliyun.com/ubuntu/ xenial universe</span><br><span class="line">deb http://mirrors.aliyun.com/ubuntu/ xenial-updates universe</span><br><span class="line">deb-src http://mirrors.aliyun.com/ubuntu/ xenial-updates universe</span><br><span class="line"></span><br><span class="line">deb http://mirrors.aliyun.com/ubuntu/ xenial-security main</span><br><span class="line">deb-src http://mirrors.aliyun.com/ubuntu/ xenial-security main</span><br><span class="line">deb http://mirrors.aliyun.com/ubuntu/ xenial-security universe</span><br><span class="line">deb-src http://mirrors.aliyun.com/ubuntu/ xenial-security universe</span><br></pre></td></tr></table></figure><h3><span id="ubuntu-an-zhuang-git">ubuntu安装git</span><a href="#ubuntu-an-zhuang-git" class="header-anchor"></a></h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">apt-get install git <span class="comment">#这里会自动安装包</span></span><br><span class="line">git --version</span><br></pre></td></tr></table></figure><h3><span id="an-zhuang-node-js">安装node js</span><a href="#an-zhuang-node-js" class="header-anchor"></a></h3><h4><span id="huo-qu-node-js-ban-ben">获取node js 版本</span><a href="#huo-qu-node-js-ban-ben" class="header-anchor"></a></h4><ol><li>打开<a href="https://nodejs.org/en/download/" target="_blank" rel="noopener">官网下载</a></li><li>找到需要下载的版本， 右键制下载链接， 这里以node-v12.3.1 为例<br><img src="https://i.loli.net/2019/06/08/5cfb56abc14f540413.png" alt="nodeJs.png"><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-get install wget</span><br><span class="line">wget https://nodejs.org/dist/v12.3.1/node-v12.3.1-linux-x64.tar.xz  </span><br><span class="line">tar -xJf node-v12.3.1-linux-x64.tar.xz -C /opt</span><br><span class="line">sudo ln -s /opt/node-v12.3.1-linux-x64/bin/node /usr/<span class="built_in">local</span>/bin/node <span class="comment">#建立软连接</span></span><br><span class="line">sudo ln -s /opt/node-v12.3.1-linux-x64/bin/npm /usr/<span class="built_in">local</span>/bin/npm <span class="comment">#建立软连接</span></span><br><span class="line">npm -v</span><br><span class="line">node -v</span><br><span class="line">sudo npm config <span class="built_in">set</span> registry https://registry.npm.taobao.org <span class="comment">#设置阿里npm数据源</span></span><br><span class="line"><span class="built_in">source</span> ~/.bashrc <span class="comment">#设置数据源立马生效</span></span><br></pre></td></tr></table></figure></li></ol><h3><span id="an-zhuang-hexo">安装hexo</span><a href="#an-zhuang-hexo" class="header-anchor"></a></h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">npm install -g hexo-cli</span><br><span class="line">sudo ln -s /opt/node-v12.3.1-linux-x64/bin/hexo /usr/<span class="built_in">local</span>/bin/hexo <span class="comment">#建立hexo软连接</span></span><br></pre></td></tr></table></figure><p>默认hexo 工作间是在/home/hexo 目录下，有需求可以自行更改</p><h3><span id="jiang-rong-qi-da-cheng-jing-xiang">将容器打成镜像</span><a href="#jiang-rong-qi-da-cheng-jing-xiang" class="header-anchor"></a></h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker commit -m <span class="string">"Add original docker hexo enviroment, inclue node js(v12.3.1) , git(2.17.1), hexo "</span> -a <span class="string">"LeiChangCheng"</span> 273901f62fb9 docker.io/changchenglei/hexo</span><br></pre></td></tr></table></figure><h3><span id="pei-zhi-tu-pian-cha-jian">配置图片插件</span><a href="#pei-zhi-tu-pian-cha-jian" class="header-anchor"></a></h3><ol><li>把主页配置文件_config.yml 里的post_asset_folder:这个选项设置为true</li><li>在根目录下输入npm install –save hexo-asset-image，这是下载安装一个可以上传本地图片的插件<br>再运行hexo new “xxxx”来生成md博文时，/source/_posts文件夹内除了xxxx.md文件还有一个同名的文件夹<br>最后在xxxx.md中想引入图片时，先把图片复制到xxxx这个文件夹中，然后只需要在xxxx.md中按照markdown的格式引入图片即可<figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">![<span class="string">Alt text</span>](<span class="link">https://i.loli.net/2019/06/08/5cfb56abc14f540413.png</span>) // 互联网查找图片</span><br><span class="line">![<span class="string">Alt text</span>](<span class="link">./nodejs.png</span>) //本地查找图片</span><br></pre></td></tr></table></figure></li></ol><h3><span id="can-kao-wen-dang">参考文档</span><a href="#can-kao-wen-dang" class="header-anchor"></a></h3><p>[1] <a href="https://hexo.io/zh-cn/docs/" target="_blank" rel="noopener">hexo配置</a><br>[2] <a href="https://hexo.io/zh-cn/docs/" target="_blank" rel="noopener">hexo建站</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;div class=&quot;toc&quot;&gt;

&lt;!-- toc --&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;#zhi-zuo-hexo-docker-jing-xiang&quot;&gt;制作HEXO Docker镜像&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;#qi-dong-docker-ron
      
    
    </summary>
    
    
      <category term="hexo" scheme="http://yoursite.com/tags/hexo/"/>
    
      <category term="docker" scheme="http://yoursite.com/tags/docker/"/>
    
  </entry>
  
</feed>
